2024-05-05 06:27:57,714	INFO resource_spec.py:212 -- Starting Ray with 35.6 GiB memory available for workers and up to 17.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2024-05-05 06:27:58,048	INFO services.py:1148 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2024-05-05 06:27:58,598 seed (final): 2766000
2024-05-05 06:27:58,611	INFO trainer.py:428 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2024-05-05 06:27:58,612	INFO trainer.py:585 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2024-05-05 06:28:07,726	INFO trainable.py:217 -- Getting current IP.
2024-05-05 06:28:07,726 ckpt_dir already exists! Planning to restore using latest snapshot from earlier (crashed) run with the same ckpt_dir ../../../runs/phase1/ckpts
2024-05-05 06:28:08,252	INFO trainable.py:217 -- Getting current IP.
2024-05-05 06:28:08,253	INFO trainable.py:423 -- Restored on 129.132.112.254 from checkpoint: ../../../runs/phase1/ckpts/latest_checkpoint.pkl
2024-05-05 06:28:08,253	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': 30000, '_time_total': 15.928747653961182, '_episodes_total': 30}
2024-05-05 06:28:08,253 load_snapshot -> loading ../../../runs/phase1/ckpts/latest_checkpoint.pkl SUCCESS for  <ray.rllib.agents.trainer_template.PPO object at 0x77a81f8fe090>
2024-05-05 06:28:18,285 Iter 6: steps this-iter 6000 total 36000 -> 30/500 episodes done
2024-05-05 06:28:20,397 Iter 7: steps this-iter 6000 total 42000 -> 30/500 episodes done
2024-05-05 06:28:22,549 Iter 8: steps this-iter 6000 total 48000 -> 30/500 episodes done
2024-05-05 06:28:24,743 Iter 9: steps this-iter 6000 total 54000 -> 30/500 episodes done
2024-05-05 06:28:27,013 Iter 10: steps this-iter 6000 total 60000 -> 60/500 episodes done
2024-05-05 06:28:27,016 custom_metrics: {}
date: 2024-05-05_06-28-27
done: false
episode_len_mean: 1000.0
episode_reward_max: 155.42046523815256
episode_reward_mean: 83.94898168795469
episode_reward_min: -3.660730120409582
episodes_this_iter: 30
episodes_total: 60
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 802.115
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.794224500656128
      entropy_coeff: 0.02500000037252903
      kl: 0.008091101422905922
      model: {}
      policy_loss: 0.0008962079882621765
      total_loss: 0.4504665732383728
      vf_explained_var: 0.06826930493116379
      vf_loss: 9.888519287109375
  load_time_ms: 362.076
  num_steps_sampled: 60000
  num_steps_trained: 240000
  sample_time_ms: 1235.775
  update_time_ms: 499.304
iterations_since_restore: 5
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.9
  gpu_util_percent0: 0.0
  ram_util_percent: 24.666666666666668
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 78.09486663835986
  p: 40.03737060970919
policy_reward_mean:
  a: 16.58164993823308
  p: 17.622381935022442
policy_reward_min:
  a: -2.928825981396642
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 2.4543506718856904
  mean_inference_ms: 2.3057334866874664
  mean_processing_ms: 0.8486852858648514
time_since_restore: 14.60966968536377
time_this_iter_s: 2.2682342529296875
time_total_s: 30.53841733932495
timestamp: 1714883307
timesteps_since_restore: 30000
timesteps_this_iter: 6000
timesteps_total: 60000
training_iteration: 10

2024-05-05 06:28:29,274 Iter 11: steps this-iter 6000 total 66000 -> 60/500 episodes done
2024-05-05 06:28:31,399 Iter 12: steps this-iter 6000 total 72000 -> 60/500 episodes done
2024-05-05 06:28:33,545 Iter 13: steps this-iter 6000 total 78000 -> 60/500 episodes done
2024-05-05 06:28:35,731 Iter 14: steps this-iter 6000 total 84000 -> 60/500 episodes done
2024-05-05 06:28:37,910 Iter 15: steps this-iter 6000 total 90000 -> 90/500 episodes done
2024-05-05 06:28:37,911 custom_metrics: {}
date: 2024-05-05_06-28-37
done: false
episode_len_mean: 1000.0
episode_reward_max: 202.71511817605582
episode_reward_mean: 115.99653581085032
episode_reward_min: 48.402981500849144
episodes_this_iter: 30
episodes_total: 90
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 724.007
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8281302452087402
      entropy_coeff: 0.02500000037252903
      kl: 0.007080732379108667
      model: {}
      policy_loss: -0.0017906129360198975
      total_loss: 0.4284420311450958
      vf_explained_var: 0.08480365574359894
      vf_loss: 9.518718719482422
  load_time_ms: 337.453
  num_steps_sampled: 90000
  num_steps_trained: 360000
  sample_time_ms: 1219.329
  update_time_ms: 251.399
iterations_since_restore: 10
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.93333333333333
  gpu_util_percent0: 0.0
  ram_util_percent: 24.633333333333336
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 124.06167132565366
  p: 55.241390243417904
policy_reward_mean:
  a: 23.160780969196615
  p: 23.35341193406376
policy_reward_min:
  a: -20.490142009914507
  p: 7.755704112995759
sampler_perf:
  mean_env_wait_ms: 2.487828043248681
  mean_inference_ms: 2.202373477949136
  mean_processing_ms: 0.8748205189067048
time_since_restore: 25.465365886688232
time_this_iter_s: 2.177255392074585
time_total_s: 41.394113540649414
timestamp: 1714883317
timesteps_since_restore: 60000
timesteps_this_iter: 6000
timesteps_total: 90000
training_iteration: 15

2024-05-05 06:28:40,135 Iter 16: steps this-iter 6000 total 96000 -> 90/500 episodes done
2024-05-05 06:28:42,512 Iter 17: steps this-iter 6000 total 102000 -> 90/500 episodes done
2024-05-05 06:28:44,719 Iter 18: steps this-iter 6000 total 108000 -> 90/500 episodes done
2024-05-05 06:28:46,863 Iter 19: steps this-iter 6000 total 114000 -> 90/500 episodes done
2024-05-05 06:28:49,051 Iter 20: steps this-iter 6000 total 120000 -> 120/500 episodes done
2024-05-05 06:28:49,053 custom_metrics: {}
date: 2024-05-05_06-28-49
done: false
episode_len_mean: 1000.0
episode_reward_max: 338.0063160147414
episode_reward_mean: 143.3941437677159
episode_reward_min: 6.939833967792964
episodes_this_iter: 30
episodes_total: 120
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 650.643
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7266055345535278
      entropy_coeff: 0.02500000037252903
      kl: 0.008310971781611443
      model: {}
      policy_loss: 0.0004543410614132881
      total_loss: 1.096581220626831
      vf_explained_var: 0.08978790044784546
      vf_loss: 22.785839080810547
  load_time_ms: 316.809
  num_steps_sampled: 120000
  num_steps_trained: 480000
  sample_time_ms: 1221.358
  update_time_ms: 3.414
iterations_since_restore: 15
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 37.23333333333333
  gpu_util_percent0: 0.0
  ram_util_percent: 24.933333333333334
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 111.41539608866304
  p: 107.44846400296917
policy_reward_mean:
  a: 26.260079215012887
  p: 38.35382690766426
policy_reward_min:
  a: -51.00272125075253
  p: 9.055273437267795
sampler_perf:
  mean_env_wait_ms: 2.5378356519625154
  mean_inference_ms: 2.1921373772168313
  mean_processing_ms: 0.8890079522442714
time_since_restore: 36.59141278266907
time_this_iter_s: 2.1865830421447754
time_total_s: 52.52016043663025
timestamp: 1714883329
timesteps_since_restore: 90000
timesteps_this_iter: 6000
timesteps_total: 120000
training_iteration: 20

2024-05-05 06:28:51,488 Iter 21: steps this-iter 6000 total 126000 -> 120/500 episodes done
2024-05-05 06:28:53,646 Iter 22: steps this-iter 6000 total 132000 -> 120/500 episodes done
2024-05-05 06:28:55,853 Iter 23: steps this-iter 6000 total 138000 -> 120/500 episodes done
2024-05-05 06:28:58,038 Iter 24: steps this-iter 6000 total 144000 -> 120/500 episodes done
2024-05-05 06:29:00,246 Iter 25: steps this-iter 6000 total 150000 -> 150/500 episodes done
2024-05-05 06:29:00,248 custom_metrics: {}
date: 2024-05-05_06-29-00
done: false
episode_len_mean: 1000.0
episode_reward_max: 445.909207669554
episode_reward_mean: 257.5607655387431
episode_reward_min: 116.94101103957256
episodes_this_iter: 30
episodes_total: 150
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 656.464
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.6555125713348389
      entropy_coeff: 0.02500000037252903
      kl: 0.004925198387354612
      model: {}
      policy_loss: -0.0014257393777370453
      total_loss: 0.21182411909103394
      vf_explained_var: 0.4468415379524231
      vf_loss: 5.0927534103393555
  load_time_ms: 317.542
  num_steps_sampled: 150000
  num_steps_trained: 600000
  sample_time_ms: 1246.993
  update_time_ms: 3.963
iterations_since_restore: 20
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.46666666666667
  gpu_util_percent0: 0.0
  ram_util_percent: 25.133333333333336
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 164.9398332141936
  p: 153.99444810797888
policy_reward_mean:
  a: 45.34660971489388
  p: 76.17432667916708
policy_reward_min:
  a: -6.256768637289511
  p: 25.26626598410957
sampler_perf:
  mean_env_wait_ms: 2.576243709566433
  mean_inference_ms: 2.1923244828771846
  mean_processing_ms: 0.898964067583212
time_since_restore: 47.77653384208679
time_this_iter_s: 2.206714391708374
time_total_s: 63.705281496047974
timestamp: 1714883340
timesteps_since_restore: 120000
timesteps_this_iter: 6000
timesteps_total: 150000
training_iteration: 25

2024-05-05 06:29:02,473 Iter 26: steps this-iter 6000 total 156000 -> 150/500 episodes done
2024-05-05 06:29:04,609 Iter 27: steps this-iter 6000 total 162000 -> 150/500 episodes done
2024-05-05 06:29:06,888 Iter 28: steps this-iter 6000 total 168000 -> 150/500 episodes done
2024-05-05 06:29:09,115 Iter 29: steps this-iter 6000 total 174000 -> 150/500 episodes done
2024-05-05 06:29:11,293 Iter 30: steps this-iter 6000 total 180000 -> 180/500 episodes done
2024-05-05 06:29:11,295 custom_metrics: {}
date: 2024-05-05_06-29-11
done: false
episode_len_mean: 1000.0
episode_reward_max: 245.73352906531534
episode_reward_mean: 115.04426189945683
episode_reward_min: 31.83543428778426
episodes_this_iter: 30
episodes_total: 180
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 668.039
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.684034824371338
      entropy_coeff: 0.02500000037252903
      kl: 0.0023993165232241154
      model: {}
      policy_loss: 0.0006564268842339516
      total_loss: 1.1831539869308472
      vf_explained_var: 0.11685214936733246
      vf_loss: 24.49197006225586
  load_time_ms: 315.299
  num_steps_sampled: 180000
  num_steps_trained: 720000
  sample_time_ms: 1225.159
  update_time_ms: 4.177
iterations_since_restore: 25
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.1
  gpu_util_percent0: 0.0
  ram_util_percent: 24.866666666666664
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 105.57671104403036
  p: 89.00486003250813
policy_reward_mean:
  a: 21.24103590054932
  p: 30.080118297259496
policy_reward_min:
  a: -23.432913926845487
  p: 9.044518723165158
sampler_perf:
  mean_env_wait_ms: 2.565720980305485
  mean_inference_ms: 2.1693246234125163
  mean_processing_ms: 0.9017923645342951
time_since_restore: 58.78379511833191
time_this_iter_s: 2.177295684814453
time_total_s: 74.71254277229309
timestamp: 1714883351
timesteps_since_restore: 150000
timesteps_this_iter: 6000
timesteps_total: 180000
training_iteration: 30

2024-05-05 06:29:13,488 Iter 31: steps this-iter 6000 total 186000 -> 180/500 episodes done
2024-05-05 06:29:15,706 Iter 32: steps this-iter 6000 total 192000 -> 180/500 episodes done
2024-05-05 06:29:18,008 Iter 33: steps this-iter 6000 total 198000 -> 180/500 episodes done
2024-05-05 06:29:20,238 Iter 34: steps this-iter 6000 total 204000 -> 180/500 episodes done
2024-05-05 06:29:22,394 Iter 35: steps this-iter 6000 total 210000 -> 210/500 episodes done
2024-05-05 06:29:22,396 custom_metrics: {}
date: 2024-05-05_06-29-22
done: false
episode_len_mean: 1000.0
episode_reward_max: 360.89289417092004
episode_reward_mean: 162.3381006034223
episode_reward_min: 45.264924091171814
episodes_this_iter: 30
episodes_total: 210
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 674.592
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7289174795150757
      entropy_coeff: 0.02500000037252903
      kl: 0.006444319151341915
      model: {}
      policy_loss: -6.312131881713867e-05
      total_loss: 2.0699071884155273
      vf_explained_var: 0.34066635370254517
      vf_loss: 42.26386260986328
  load_time_ms: 320.283
  num_steps_sampled: 210000
  num_steps_trained: 840000
  sample_time_ms: 1200.565
  update_time_ms: 3.971
iterations_since_restore: 30
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 47.5
  gpu_util_percent0: 0.0
  ram_util_percent: 25.0
  vram_util_percent0: 0.007775606578733106
pid: 105442
policy_reward_max:
  a: 137.60471378630265
  p: 165.98438922515152
policy_reward_mean:
  a: 25.496919267192343
  p: 60.35042353465288
policy_reward_min:
  a: -73.94568794040244
  p: 7.7557041129956765
sampler_perf:
  mean_env_wait_ms: 2.562817233089393
  mean_inference_ms: 2.157221058703181
  mean_processing_ms: 0.9025864764557199
time_since_restore: 69.83408951759338
time_this_iter_s: 2.1544620990753174
time_total_s: 85.76283717155457
timestamp: 1714883362
timesteps_since_restore: 180000
timesteps_this_iter: 6000
timesteps_total: 210000
training_iteration: 35

2024-05-05 06:29:24,578 Iter 36: steps this-iter 6000 total 216000 -> 210/500 episodes done
2024-05-05 06:29:26,880 Iter 37: steps this-iter 6000 total 222000 -> 210/500 episodes done
2024-05-05 06:29:29,312 Iter 38: steps this-iter 6000 total 228000 -> 210/500 episodes done
2024-05-05 06:29:31,665 Iter 39: steps this-iter 6000 total 234000 -> 210/500 episodes done
2024-05-05 06:29:33,937 Iter 40: steps this-iter 6000 total 240000 -> 240/500 episodes done
2024-05-05 06:29:33,939 custom_metrics: {}
date: 2024-05-05_06-29-33
done: false
episode_len_mean: 1000.0
episode_reward_max: 565.2434329848137
episode_reward_mean: 340.39559912622985
episode_reward_min: 114.7596749371939
episodes_this_iter: 30
episodes_total: 240
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 676.098
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8731293678283691
      entropy_coeff: 0.02500000037252903
      kl: 0.00762709928676486
      model: {}
      policy_loss: -0.0005698017776012421
      total_loss: 0.8591161966323853
      vf_explained_var: 0.6034960746765137
      vf_loss: 18.130285263061523
  load_time_ms: 317.969
  num_steps_sampled: 240000
  num_steps_trained: 960000
  sample_time_ms: 1254.32
  update_time_ms: 3.86
iterations_since_restore: 35
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.63333333333333
  gpu_util_percent0: 0.0
  ram_util_percent: 24.933333333333334
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 140.24843982468929
  p: 244.2450932180006
policy_reward_mean:
  a: 55.27619883230754
  p: 119.2908037969995
policy_reward_min:
  a: -1.7150813826386653
  p: 36.531032101460745
sampler_perf:
  mean_env_wait_ms: 2.577064952606054
  mean_inference_ms: 2.1800968287496603
  mean_processing_ms: 0.9015793903199899
time_since_restore: 81.36765813827515
time_this_iter_s: 2.270704984664917
time_total_s: 97.29640579223633
timestamp: 1714883373
timesteps_since_restore: 210000
timesteps_this_iter: 6000
timesteps_total: 240000
training_iteration: 40

2024-05-05 06:29:36,130 Iter 41: steps this-iter 6000 total 246000 -> 240/500 episodes done
2024-05-05 06:29:38,308 Iter 42: steps this-iter 6000 total 252000 -> 240/500 episodes done
2024-05-05 06:29:40,528 Iter 43: steps this-iter 6000 total 258000 -> 240/500 episodes done
2024-05-05 06:29:42,743 Iter 44: steps this-iter 6000 total 264000 -> 240/500 episodes done
2024-05-05 06:29:44,954 Iter 45: steps this-iter 6000 total 270000 -> 270/500 episodes done
2024-05-05 06:29:44,955 custom_metrics: {}
date: 2024-05-05_06-29-44
done: false
episode_len_mean: 1000.0
episode_reward_max: 458.3850929263057
episode_reward_mean: 257.7106777197457
episode_reward_min: 77.54290766859657
episodes_this_iter: 30
episodes_total: 270
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 673.081
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.833674669265747
      entropy_coeff: 0.02500000037252903
      kl: 0.004463213495910168
      model: {}
      policy_loss: -0.00021572131663560867
      total_loss: 4.293941020965576
      vf_explained_var: 0.33579716086387634
      vf_loss: 86.79997253417969
  load_time_ms: 314.009
  num_steps_sampled: 270000
  num_steps_trained: 1080000
  sample_time_ms: 1256.681
  update_time_ms: 3.654
iterations_since_restore: 40
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.833333333333336
  gpu_util_percent0: 0.0
  ram_util_percent: 25.333333333333332
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 189.69675158356324
  p: 217.0407246479823
policy_reward_mean:
  a: 34.45304204573487
  p: 119.89850953680613
policy_reward_min:
  a: -78.32566844719264
  p: 28.93208629590499
sampler_perf:
  mean_env_wait_ms: 2.57146116227392
  mean_inference_ms: 2.1673119952667457
  mean_processing_ms: 0.9016922939341699
time_since_restore: 92.37050771713257
time_this_iter_s: 2.204918146133423
time_total_s: 108.29925537109375
timestamp: 1714883384
timesteps_since_restore: 240000
timesteps_this_iter: 6000
timesteps_total: 270000
training_iteration: 45

2024-05-05 06:29:47,110 Iter 46: steps this-iter 6000 total 276000 -> 270/500 episodes done
2024-05-05 06:29:49,288 Iter 47: steps this-iter 6000 total 282000 -> 270/500 episodes done
2024-05-05 06:29:51,454 Iter 48: steps this-iter 6000 total 288000 -> 270/500 episodes done
2024-05-05 06:29:53,641 Iter 49: steps this-iter 6000 total 294000 -> 270/500 episodes done
2024-05-05 06:29:55,925 Iter 50: steps this-iter 6000 total 300000 -> 300/500 episodes done
2024-05-05 06:29:55,927 custom_metrics: {}
date: 2024-05-05_06-29-55
done: false
episode_len_mean: 1000.0
episode_reward_max: 630.3203711219829
episode_reward_mean: 325.1639351718999
episode_reward_min: 103.3282384876602
episodes_this_iter: 30
episodes_total: 300
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 668.69
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.876359224319458
      entropy_coeff: 0.02500000037252903
      kl: 0.006679967977106571
      model: {}
      policy_loss: -0.0006387233734130859
      total_loss: 3.475797414779663
      vf_explained_var: 0.4207303524017334
      vf_loss: 70.46690368652344
  load_time_ms: 316.676
  num_steps_sampled: 300000
  num_steps_trained: 1200000
  sample_time_ms: 1201.464
  update_time_ms: 3.445
iterations_since_restore: 45
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.43333333333333
  gpu_util_percent0: 0.0
  ram_util_percent: 25.166666666666668
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 219.37721530290273
  p: 271.49576936111504
policy_reward_mean:
  a: 46.87229822231481
  p: 137.67474228264078
policy_reward_min:
  a: -41.67053238667877
  p: 33.48651337829806
sampler_perf:
  mean_env_wait_ms: 2.5656051389933463
  mean_inference_ms: 2.1557475939303092
  mean_processing_ms: 0.9013563130770076
time_since_restore: 103.33290457725525
time_this_iter_s: 2.2827696800231934
time_total_s: 119.26165223121643
timestamp: 1714883395
timesteps_since_restore: 270000
timesteps_this_iter: 6000
timesteps_total: 300000
training_iteration: 50

2024-05-05 06:29:58,219 Iter 51: steps this-iter 6000 total 306000 -> 300/500 episodes done
2024-05-05 06:30:00,501 Iter 52: steps this-iter 6000 total 312000 -> 300/500 episodes done
2024-05-05 06:30:02,715 Iter 53: steps this-iter 6000 total 318000 -> 300/500 episodes done
2024-05-05 06:30:05,027 Iter 54: steps this-iter 6000 total 324000 -> 300/500 episodes done
2024-05-05 06:30:07,174 Iter 55: steps this-iter 6000 total 330000 -> 330/500 episodes done
2024-05-05 06:30:07,176 custom_metrics: {}
date: 2024-05-05_06-30-07
done: false
episode_len_mean: 1000.0
episode_reward_max: 499.1702709076739
episode_reward_mean: 304.3422684690339
episode_reward_min: 83.9984322260315
episodes_this_iter: 30
episodes_total: 330
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 677.88
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.837705135345459
      entropy_coeff: 0.02500000037252903
      kl: 0.006303808651864529
      model: {}
      policy_loss: -0.00041226111352443695
      total_loss: 2.9642035961151123
      vf_explained_var: 0.4071609377861023
      vf_loss: 60.2111701965332
  load_time_ms: 322.047
  num_steps_sampled: 330000
  num_steps_trained: 1320000
  sample_time_ms: 1210.729
  update_time_ms: 3.395
iterations_since_restore: 50
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 47.3
  gpu_util_percent0: 0.0
  ram_util_percent: 25.1
  vram_util_percent0: 0.007775606578733106
pid: 105442
policy_reward_max:
  a: 125.70651801998102
  p: 205.97371233573904
policy_reward_mean:
  a: 43.24613652389942
  p: 131.35772237343653
policy_reward_min:
  a: -55.6279481513854
  p: 43.95977850985645
sampler_perf:
  mean_env_wait_ms: 2.571235796342241
  mean_inference_ms: 2.154547237727704
  mean_processing_ms: 0.9042588662327208
time_since_restore: 114.57157063484192
time_this_iter_s: 2.1452925205230713
time_total_s: 130.5003182888031
timestamp: 1714883407
timesteps_since_restore: 300000
timesteps_this_iter: 6000
timesteps_total: 330000
training_iteration: 55

2024-05-05 06:30:09,363 Iter 56: steps this-iter 6000 total 336000 -> 330/500 episodes done
2024-05-05 06:30:11,767 Iter 57: steps this-iter 6000 total 342000 -> 330/500 episodes done
2024-05-05 06:30:13,954 Iter 58: steps this-iter 6000 total 348000 -> 330/500 episodes done
2024-05-05 06:30:16,401 Iter 59: steps this-iter 6000 total 354000 -> 330/500 episodes done
2024-05-05 06:30:18,587 Iter 60: steps this-iter 6000 total 360000 -> 360/500 episodes done
2024-05-05 06:30:18,589 custom_metrics: {}
date: 2024-05-05_06-30-18
done: false
episode_len_mean: 1000.0
episode_reward_max: 434.6807862400924
episode_reward_mean: 235.4307264386875
episode_reward_min: 45.93989606606938
episodes_this_iter: 30
episodes_total: 360
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 680.457
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8103117942810059
      entropy_coeff: 0.02500000037252903
      kl: 0.005775828845798969
      model: {}
      policy_loss: -0.0015590060502290726
      total_loss: 4.923544406890869
      vf_explained_var: 0.09141237288713455
      vf_loss: 99.40721893310547
  load_time_ms: 320.123
  num_steps_sampled: 360000
  num_steps_trained: 1440000
  sample_time_ms: 1253.761
  update_time_ms: 3.544
iterations_since_restore: 55
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.56666666666667
  gpu_util_percent0: 0.0
  ram_util_percent: 24.96666666666667
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 150.52841312283877
  p: 192.74952216501632
policy_reward_mean:
  a: 32.02606053598638
  p: 107.32648429474213
policy_reward_min:
  a: -79.79494392772857
  p: 35.392950277546056
sampler_perf:
  mean_env_wait_ms: 2.5886516919537854
  mean_inference_ms: 2.1618823472273747
  mean_processing_ms: 0.9068402153750038
time_since_restore: 125.97374320030212
time_this_iter_s: 2.1844286918640137
time_total_s: 141.9024908542633
timestamp: 1714883418
timesteps_since_restore: 330000
timesteps_this_iter: 6000
timesteps_total: 360000
training_iteration: 60

2024-05-05 06:30:21,038 Iter 61: steps this-iter 6000 total 366000 -> 360/500 episodes done
2024-05-05 06:30:23,178 Iter 62: steps this-iter 6000 total 372000 -> 360/500 episodes done
2024-05-05 06:30:25,474 Iter 63: steps this-iter 6000 total 378000 -> 360/500 episodes done
2024-05-05 06:30:27,823 Iter 64: steps this-iter 6000 total 384000 -> 360/500 episodes done
2024-05-05 06:30:29,989 Iter 65: steps this-iter 6000 total 390000 -> 390/500 episodes done
2024-05-05 06:30:29,991 custom_metrics: {}
date: 2024-05-05_06-30-29
done: false
episode_len_mean: 1000.0
episode_reward_max: 276.27655760398216
episode_reward_mean: 120.10410324136414
episode_reward_min: 3.479438275906574
episodes_this_iter: 30
episodes_total: 390
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 668.57
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8470962047576904
      entropy_coeff: 0.02500000037252903
      kl: 0.0035883202217519283
      model: {}
      policy_loss: 0.00037918053567409515
      total_loss: 2.5472652912139893
      vf_explained_var: 0.3621947467327118
      vf_loss: 51.86126708984375
  load_time_ms: 312.676
  num_steps_sampled: 390000
  num_steps_trained: 1560000
  sample_time_ms: 1287.851
  update_time_ms: 3.769
iterations_since_restore: 60
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 35.733333333333334
  gpu_util_percent0: 0.0
  ram_util_percent: 24.933333333333334
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 105.03343126556378
  p: 147.97441065691444
policy_reward_mean:
  a: 15.188943562944731
  p: 59.3483289895853
policy_reward_min:
  a: -47.15351610964521
  p: 8.665820311797324
sampler_perf:
  mean_env_wait_ms: 2.6018592174479074
  mean_inference_ms: 2.1678767707438396
  mean_processing_ms: 0.9145408088199444
time_since_restore: 137.36473393440247
time_this_iter_s: 2.1641502380371094
time_total_s: 153.29348158836365
timestamp: 1714883429
timesteps_since_restore: 360000
timesteps_this_iter: 6000
timesteps_total: 390000
training_iteration: 65

2024-05-05 06:30:32,333 Iter 66: steps this-iter 6000 total 396000 -> 390/500 episodes done
2024-05-05 06:30:34,730 Iter 67: steps this-iter 6000 total 402000 -> 390/500 episodes done
2024-05-05 06:30:36,922 Iter 68: steps this-iter 6000 total 408000 -> 390/500 episodes done
2024-05-05 06:30:39,394 Iter 69: steps this-iter 6000 total 414000 -> 390/500 episodes done
2024-05-05 06:30:41,582 Iter 70: steps this-iter 6000 total 420000 -> 420/500 episodes done
2024-05-05 06:30:41,584 custom_metrics: {}
date: 2024-05-05_06-30-41
done: false
episode_len_mean: 1000.0
episode_reward_max: 542.2397396810147
episode_reward_mean: 377.6331560232623
episode_reward_min: 238.84264075832476
episodes_this_iter: 30
episodes_total: 420
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 669.128
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.9651297330856323
      entropy_coeff: 0.02500000037252903
      kl: 0.00430638063699007
      model: {}
      policy_loss: -0.0007348470389842987
      total_loss: 3.3139405250549316
      vf_explained_var: 0.33639052510261536
      vf_loss: 67.27607727050781
  load_time_ms: 311.708
  num_steps_sampled: 420000
  num_steps_trained: 1680000
  sample_time_ms: 1306.649
  update_time_ms: 3.671
iterations_since_restore: 65
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.733333333333334
  gpu_util_percent0: 0.0
  ram_util_percent: 25.0
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 137.95773939734423
  p: 238.11288923343676
policy_reward_mean:
  a: 49.048006860855466
  p: 181.44112857984018
policy_reward_min:
  a: -45.17338829372375
  p: 112.56527668684016
sampler_perf:
  mean_env_wait_ms: 2.62108329589271
  mean_inference_ms: 2.1771646605312336
  mean_processing_ms: 0.9184592522599954
time_since_restore: 148.94713592529297
time_this_iter_s: 2.1860122680664062
time_total_s: 164.87588357925415
timestamp: 1714883441
timesteps_since_restore: 390000
timesteps_this_iter: 6000
timesteps_total: 420000
training_iteration: 70

2024-05-05 06:30:43,838 Iter 71: steps this-iter 6000 total 426000 -> 420/500 episodes done
2024-05-05 06:30:46,068 Iter 72: steps this-iter 6000 total 432000 -> 420/500 episodes done
2024-05-05 06:30:48,336 Iter 73: steps this-iter 6000 total 438000 -> 420/500 episodes done
2024-05-05 06:30:50,726 Iter 74: steps this-iter 6000 total 444000 -> 420/500 episodes done
2024-05-05 06:30:52,937 Iter 75: steps this-iter 6000 total 450000 -> 450/500 episodes done
2024-05-05 06:30:52,939 custom_metrics: {}
date: 2024-05-05_06-30-52
done: false
episode_len_mean: 1000.0
episode_reward_max: 369.98273859446937
episode_reward_mean: 261.6708203012901
episode_reward_min: 150.8206536861931
episodes_this_iter: 30
episodes_total: 450
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 673.781
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.8836212158203125
      entropy_coeff: 0.02500000037252903
      kl: 0.00592900300398469
      model: {}
      policy_loss: 0.0007110121659934521
      total_loss: 3.456247329711914
      vf_explained_var: 0.392322301864624
      vf_loss: 70.05253601074219
  load_time_ms: 320.793
  num_steps_sampled: 450000
  num_steps_trained: 1800000
  sample_time_ms: 1287.949
  update_time_ms: 3.672
iterations_since_restore: 70
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 37.266666666666666
  gpu_util_percent0: 0.0
  ram_util_percent: 25.333333333333332
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 134.36298762534085
  p: 201.17658538793083
policy_reward_mean:
  a: 33.487827695116756
  p: 127.71950952082317
policy_reward_min:
  a: -53.07058823591298
  p: 50.60677576907127
sampler_perf:
  mean_env_wait_ms: 2.6251926049304526
  mean_inference_ms: 2.178187245763864
  mean_processing_ms: 0.9198013159262806
time_since_restore: 160.28922605514526
time_this_iter_s: 2.2093732357025146
time_total_s: 176.21797370910645
timestamp: 1714883452
timesteps_since_restore: 420000
timesteps_this_iter: 6000
timesteps_total: 450000
training_iteration: 75

2024-05-05 06:30:55,235 Iter 76: steps this-iter 6000 total 456000 -> 450/500 episodes done
2024-05-05 06:30:57,559 Iter 77: steps this-iter 6000 total 462000 -> 450/500 episodes done
2024-05-05 06:30:59,764 Iter 78: steps this-iter 6000 total 468000 -> 450/500 episodes done
2024-05-05 06:31:02,010 Iter 79: steps this-iter 6000 total 474000 -> 450/500 episodes done
2024-05-05 06:31:04,203 Iter 80: steps this-iter 6000 total 480000 -> 480/500 episodes done
2024-05-05 06:31:04,205 custom_metrics: {}
date: 2024-05-05_06-31-04
done: false
episode_len_mean: 1000.0
episode_reward_max: 538.6006615930067
episode_reward_mean: 377.01240770501676
episode_reward_min: 173.8276397097491
episodes_this_iter: 30
episodes_total: 480
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 677.972
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.836270809173584
      entropy_coeff: 0.02500000037252903
      kl: 0.005148167721927166
      model: {}
      policy_loss: -7.264409214258194e-05
      total_loss: 2.953747034072876
      vf_explained_var: 0.5073127746582031
      vf_loss: 59.994529724121094
  load_time_ms: 324.612
  num_steps_sampled: 480000
  num_steps_trained: 1920000
  sample_time_ms: 1242.659
  update_time_ms: 3.75
iterations_since_restore: 75
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 37.1
  gpu_util_percent0: 0.0
  ram_util_percent: 24.96666666666667
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 145.1152633858075
  p: 239.68120819832373
policy_reward_mean:
  a: 50.89873856162699
  p: 173.41745345850902
policy_reward_min:
  a: -54.64357045062923
  p: 87.25084862925863
sampler_perf:
  mean_env_wait_ms: 2.6239185177707447
  mean_inference_ms: 2.1743658476187577
  mean_processing_ms: 0.9204582148407989
time_since_restore: 171.50005745887756
time_this_iter_s: 2.1912355422973633
time_total_s: 187.42880511283875
timestamp: 1714883464
timesteps_since_restore: 450000
timesteps_this_iter: 6000
timesteps_total: 480000
training_iteration: 80

2024-05-05 06:31:06,399 Iter 81: steps this-iter 6000 total 486000 -> 480/500 episodes done
2024-05-05 06:31:08,778 Iter 82: steps this-iter 6000 total 492000 -> 480/500 episodes done
2024-05-05 06:31:10,954 Iter 83: steps this-iter 6000 total 498000 -> 480/500 episodes done
2024-05-05 06:31:13,416 Iter 84: steps this-iter 6000 total 504000 -> 480/500 episodes done
2024-05-05 06:31:15,632 Iter 85: steps this-iter 6000 total 510000 -> 510/500 episodes done
2024-05-05 06:31:15,634 custom_metrics: {}
date: 2024-05-05_06-31-15
done: false
episode_len_mean: 1000.0
episode_reward_max: 528.7805490578467
episode_reward_mean: 337.6062865559705
episode_reward_min: 150.99020632267383
episodes_this_iter: 30
episodes_total: 510
experiment_id: ae6d2198f48a40edafa6fb6f48eaf56f
hostname: wks
info:
  grad_time_ms: 676.781
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.7721304893493652
      entropy_coeff: 0.02500000037252903
      kl: 0.004109695088118315
      model: {}
      policy_loss: 0.0001402776688337326
      total_loss: 3.882052183151245
      vf_explained_var: 0.28197181224823
      vf_loss: 78.52430725097656
  load_time_ms: 314.404
  num_steps_sampled: 510000
  num_steps_trained: 2040000
  sample_time_ms: 1262.127
  update_time_ms: 3.296
iterations_since_restore: 80
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 36.93333333333333
  gpu_util_percent0: 0.0
  ram_util_percent: 25.0
  vram_util_percent0: 0.007775606578733105
pid: 105442
policy_reward_max:
  a: 148.96683458986655
  p: 236.88631703838809
policy_reward_mean:
  a: 42.350894080609336
  p: 168.2027102335329
policy_reward_min:
  a: -40.0347986584368
  p: 77.32763149472706
sampler_perf:
  mean_env_wait_ms: 2.6345273125900035
  mean_inference_ms: 2.178998903674617
  mean_processing_ms: 0.9211107012882997
time_since_restore: 182.91833400726318
time_this_iter_s: 2.2148447036743164
time_total_s: 198.84708166122437
timestamp: 1714883475
timesteps_since_restore: 480000
timesteps_this_iter: 6000
timesteps_total: 510000
training_iteration: 85

2024-05-05 06:31:15,634 Completing! Saving final snapshot...


2024-05-05 06:31:15,749 Saved Trainer snapshot + Env object @ ../../../runs/phase1/ckpts/latest_checkpoint.pkl
2024-05-05 06:31:15,842 Saved TF weights @ ../../../runs/phase1/ckpts/agent.policy-model-weight-array.global-step-510000
2024-05-05 06:31:15,919 Saved TF weights @ ../../../runs/phase1/ckpts/planner.policy-model-weight-array.global-step-510000
2024-05-05 06:31:15,919 Final snapshot saved! All done.
agent.tf.weights.global-step-30000
{'agent_policy': {'clip_param': 0.3, 'entropy_coeff': 0.025, 'entropy_coeff_schedule': None, 'gamma': 0.998, 'grad_clip': 10.0, 'kl_coeff': 0.0, 'kl_target': 0.01, 'lambda': 0.98, 'lr': 0.0003, 'lr_schedule': None, 'model': {'custom_model': 'keras_conv_lstm', 'custom_options': {'fc_dim': 128, 'idx_emb_dim': 4, 'input_emb_vocab': 100, 'lstm_cell_size': 128, 'num_conv': 2, 'num_fc': 2}, 'max_seq_len': 25}, 'use_gae': True, 'vf_clip_param': 50.0, 'vf_loss_coeff': 0.05, 'vf_share_layers': False}, 'env': {'components': [{'Build': {'build_labor': 10, 'payment': 10, 'payment_max_skill_multiplier': 3, 'skill_dist': 'pareto'}}, {'ContinuousDoubleAuction': {'max_bid_ask': 10, 'max_num_orders': 5, 'order_duration': 50, 'order_labor': 0.25}}, {'Gather': {'collect_labor': 1, 'move_labor': 1, 'skill_dist': 'pareto'}}, {'PeriodicBracketTax': {'bracket_spacing': 'us-federal', 'disable_taxes': False, 'period': 100, 'rate_disc': 0.05, 'tax_annealing_schedule': [-100, 0.001], 'tax_model': 'model_wrapper', 'usd_scaling': 1000}}], 'dense_log_frequency': 20, 'energy_cost': 0.21, 'energy_warmup_constant': 0, 'env_layout_file': 'quadrant_25x25_20each_30clump.txt', 'env_weighting': [0, 1, 0.2, 0.4], 'episode_length': 1000, 'equ_weighting': [-2, 2, 4, 8], 'fixed_four_skill_and_loc': True, 'flatten_masks': True, 'flatten_observations': True, 'isoelastic_eta': 0.23, 'mobile_agent_class': 'HeteroMobileAgent', 'multi_action_mode_agents': False, 'multi_action_mode_planner': True, 'n_agents': 4, 'planner_gets_spatial_info': False, 'planner_reward_type': 'coin_eq_times_productivity', 'scenario_name': 'layout_from_file/hetero_agents', 'starting_agent_coin': 0, 'world_size': [25, 25]}, 'general': {'ckpt_frequency_steps': 20000000, 'cpus': 15, 'episodes': 2000, 'gpus': 0, 'restore_tf_weights_agents': '/home/bahain/forl/runs/phase1/ckpts/agent.tf.weights.global-step-30000', 'restore_tf_weights_planner': '', 'train_planner': True}, 'planner_policy': {'clip_param': 0.3, 'entropy_coeff': 0.125, 'entropy_coeff_schedule': [[0, 2.0], [50000000, 0.125]], 'gamma': 0.998, 'grad_clip': 10.0, 'kl_coeff': 0.0, 'kl_target': 0.01, 'lambda': 0.98, 'lr': 0.0001, 'lr_schedule': None, 'model': {'custom_model': 'keras_conv_lstm', 'custom_options': {'fc_dim': 256, 'idx_emb_dim': 4, 'input_emb_vocab': 100, 'lstm_cell_size': 256, 'num_conv': 2, 'num_fc': 2}, 'max_seq_len': 25}, 'use_gae': True, 'vf_clip_param': 50.0, 'vf_loss_coeff': 0.05, 'vf_share_layers': False}, 'trainer': {'batch_mode': 'truncate_episodes', 'env_config': None, 'local_tf_session_args': {'inter_op_parallelism_threads': 2, 'intra_op_parallelism_threads': 24}, 'metrics_smoothing_episodes': None, 'multiagent': None, 'no_done_at_end': False, 'num_envs_per_worker': 2, 'num_gpus': 0, 'num_gpus_per_worker': 0, 'num_sgd_iter': 1, 'num_workers': 15, 'observation_filter': 'NoFilter', 'rollout_fragment_length': 200, 'seed': None, 'sgd_minibatch_size': 1500, 'shuffle_sequences': True, 'tf_session_args': {'allow_soft_placement': True, 'device_count': {'CPU': 15, 'GPU': 0}, 'gpu_options': {'allow_growth': True}, 'inter_op_parallelism_threads': 2, 'intra_op_parallelism_threads': 24, 'log_device_placement': False}, 'train_batch_size': 6000}}
agent.tf.weights.global-step-30000
Filename added to configure.yaml successfully.
2024-05-05 06:31:18,470	INFO resource_spec.py:212 -- Starting Ray with 35.64 GiB memory available for workers and up to 17.83 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2024-05-05 06:31:18,807	INFO services.py:1148 -- View the Ray dashboard at [1m[32mlocalhost:8265[39m[22m
2024-05-05 06:31:19,358 seed (final): 2967000
2024-05-05 06:31:19,372	INFO trainer.py:428 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2024-05-05 06:31:19,372	INFO trainer.py:585 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2024-05-05 06:31:39,570	INFO trainable.py:180 -- _setup took 20.198 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2024-05-05 06:31:39,570	INFO trainable.py:217 -- Getting current IP.
2024-05-05 06:31:39,571 ckpt_dir already exists! Planning to restore using latest snapshot from earlier (crashed) run with the same ckpt_dir ../../../runs/phase2/ckpts
2024-05-05 06:31:39,974	INFO trainable.py:217 -- Getting current IP.
2024-05-05 06:31:39,974	INFO trainable.py:423 -- Restored on 129.132.112.254 from checkpoint: ../../../runs/phase2/ckpts/latest_checkpoint.pkl
2024-05-05 06:31:39,974	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': 30000, '_time_total': 28.50905990600586, '_episodes_total': 30}
2024-05-05 06:31:39,974 load_snapshot -> loading ../../../runs/phase2/ckpts/latest_checkpoint.pkl SUCCESS for  <ray.rllib.agents.trainer_template.PPO object at 0x753d14446250>
2024-05-05 06:31:56,798 Iter 6: steps this-iter 6000 total 36000 -> 30/2000 episodes done
2024-05-05 06:31:59,761 Iter 7: steps this-iter 6000 total 42000 -> 30/2000 episodes done
2024-05-05 06:32:02,896 Iter 8: steps this-iter 6000 total 48000 -> 30/2000 episodes done
2024-05-05 06:32:05,912 Iter 9: steps this-iter 6000 total 54000 -> 30/2000 episodes done
2024-05-05 06:32:08,942 Iter 10: steps this-iter 6000 total 60000 -> 60/2000 episodes done
2024-05-05 06:32:08,943 custom_metrics: {}
date: 2024-05-05_06-32-08
done: false
episode_len_mean: 1000.0
episode_reward_max: -233.19673809718554
episode_reward_mean: -292.89379115032114
episode_reward_min: -333.6071481093206
episodes_this_iter: 30
episodes_total: 60
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1272.125
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 1.0811587572097778
      entropy_coeff: 0.02500000037252903
      kl: 0.001965203555300832
      model: {}
      policy_loss: -0.00039460137486457825
      total_loss: 1.3257825374603271
      vf_explained_var: -5.840137600898743e-05
      vf_loss: 27.06412124633789
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0962384045124054
      entropy_coeff: 1.9979749917984009
      kl: 6.937522812222596e-06
      model: {}
      policy_loss: 4.441477358341217e-06
      total_loss: -0.10390212386846542
      vf_explained_var: 0.14586442708969116
      vf_loss: 1.7675070762634277
  load_time_ms: 463.936
  num_steps_sampled: 60000
  num_steps_trained: 60000
  sample_time_ms: 1815.956
  update_time_ms: 1887.78
iterations_since_restore: 5
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 42.575
  gpu_util_percent0: 0.0
  ram_util_percent: 28.325
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: -43.50231636185018
  p: 22.637632427565055
policy_reward_mean:
  a: -75.44565510940765
  p: 8.8888292873121
policy_reward_min:
  a: -90.71353958869082
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 2.9357589883007202
  mean_inference_ms: 4.28093941339524
  mean_processing_ms: 1.071197399885068
time_since_restore: 27.59426760673523
time_this_iter_s: 3.027733087539673
time_total_s: 56.10332751274109
timestamp: 1714883528
timesteps_since_restore: 30000
timesteps_this_iter: 6000
timesteps_total: 60000
training_iteration: 10

2024-05-05 06:32:11,966 Iter 11: steps this-iter 6000 total 66000 -> 60/2000 episodes done
2024-05-05 06:32:15,246 Iter 12: steps this-iter 6000 total 72000 -> 60/2000 episodes done
2024-05-05 06:32:18,248 Iter 13: steps this-iter 6000 total 78000 -> 60/2000 episodes done
2024-05-05 06:32:21,248 Iter 14: steps this-iter 6000 total 84000 -> 60/2000 episodes done
2024-05-05 06:32:24,467 Iter 15: steps this-iter 6000 total 90000 -> 90/2000 episodes done
2024-05-05 06:32:24,468 custom_metrics: {}
date: 2024-05-05_06-32-24
done: false
episode_len_mean: 1000.0
episode_reward_max: -89.60164765937641
episode_reward_mean: -117.55523451722496
episode_reward_min: -147.68250000000126
episodes_this_iter: 30
episodes_total: 90
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1122.973
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.6374694108963013
      entropy_coeff: 0.02500000037252903
      kl: 0.002094350289553404
      model: {}
      policy_loss: -0.000125981867313385
      total_loss: 1.172189474105835
      vf_explained_var: 9.223446249961853e-05
      vf_loss: 23.765045166015625
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09652163833379745
      entropy_coeff: 1.9968500137329102
      kl: 3.48114599546534e-06
      model: {}
      policy_loss: -8.177012205123901e-06
      total_loss: -0.14545489847660065
      vf_explained_var: 0.06817278265953064
      vf_loss: 0.945850133895874
  load_time_ms: 419.609
  num_steps_sampled: 90000
  num_steps_trained: 90000
  sample_time_ms: 1777.464
  update_time_ms: 947.351
iterations_since_restore: 10
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.949999999999996
  gpu_util_percent0: 0.0
  ram_util_percent: 29.075000000000003
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: -12.582758578780876
  p: 16.35855050324106
policy_reward_mean:
  a: -31.16797024537259
  p: 7.1166464642659
policy_reward_min:
  a: -44.33671793281407
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 2.9863494963282924
  mean_inference_ms: 4.122120353473776
  mean_processing_ms: 1.0980408846607175
time_since_restore: 43.10880756378174
time_this_iter_s: 3.2160186767578125
time_total_s: 71.6178674697876
timestamp: 1714883544
timesteps_since_restore: 60000
timesteps_this_iter: 6000
timesteps_total: 90000
training_iteration: 15

2024-05-05 06:32:27,546 Iter 16: steps this-iter 6000 total 96000 -> 90/2000 episodes done
2024-05-05 06:32:30,722 Iter 17: steps this-iter 6000 total 102000 -> 90/2000 episodes done
2024-05-05 06:32:33,770 Iter 18: steps this-iter 6000 total 108000 -> 90/2000 episodes done
2024-05-05 06:32:36,761 Iter 19: steps this-iter 6000 total 114000 -> 90/2000 episodes done
2024-05-05 06:32:39,757 Iter 20: steps this-iter 6000 total 120000 -> 120/2000 episodes done
2024-05-05 06:32:39,759 custom_metrics: {}
date: 2024-05-05_06-32-39
done: false
episode_len_mean: 1000.0
episode_reward_max: -27.272956652981893
episode_reward_mean: -57.40312054434976
episode_reward_min: -78.80249999999997
episodes_this_iter: 30
episodes_total: 120
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 971.257
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.5233272910118103
      entropy_coeff: 0.02500000037252903
      kl: 0.001448251074180007
      model: {}
      policy_loss: -0.00044273585081100464
      total_loss: 0.5491867065429688
      vf_explained_var: 0.011005256325006485
      vf_loss: 11.254251480102539
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09672887623310089
      entropy_coeff: 1.9957250356674194
      kl: 5.055608653492527e-06
      model: {}
      policy_loss: -5.83939254283905e-06
      total_loss: -0.16914626955986023
      vf_explained_var: 0.09219434857368469
      vf_loss: 0.4780762493610382
  load_time_ms: 370.372
  num_steps_sampled: 120000
  num_steps_trained: 120000
  sample_time_ms: 1720.041
  update_time_ms: 7.305
iterations_since_restore: 15
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 50.93333333333333
  gpu_util_percent0: 0.0
  ram_util_percent: 29.099999999999998
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 1.833122500547148
  p: 13.156743647352442
policy_reward_mean:
  a: -15.688339610339488
  p: 5.350237897008216
policy_reward_min:
  a: -25.525254667588335
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 2.980863069489601
  mean_inference_ms: 4.043722809466683
  mean_processing_ms: 1.1078805244460206
time_since_restore: 58.36401057243347
time_this_iter_s: 2.9942030906677246
time_total_s: 86.87307047843933
timestamp: 1714883559
timesteps_since_restore: 90000
timesteps_this_iter: 6000
timesteps_total: 120000
training_iteration: 20

2024-05-05 06:32:42,763 Iter 21: steps this-iter 6000 total 126000 -> 120/2000 episodes done
2024-05-05 06:32:45,952 Iter 22: steps this-iter 6000 total 132000 -> 120/2000 episodes done
2024-05-05 06:32:48,953 Iter 23: steps this-iter 6000 total 138000 -> 120/2000 episodes done
2024-05-05 06:32:51,931 Iter 24: steps this-iter 6000 total 144000 -> 120/2000 episodes done
2024-05-05 06:32:54,911 Iter 25: steps this-iter 6000 total 150000 -> 150/2000 episodes done
2024-05-05 06:32:54,913 custom_metrics: {}
date: 2024-05-05_06-32-54
done: false
episode_len_mean: 1000.0
episode_reward_max: -21.25634645485956
episode_reward_mean: -38.130756832524064
episode_reward_min: -55.370710302298384
episodes_this_iter: 30
episodes_total: 150
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 963.586
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.39399197697639465
      entropy_coeff: 0.02500000037252903
      kl: 0.0021024232264608145
      model: {}
      policy_loss: 0.00013715587556362152
      total_loss: 0.08749006688594818
      vf_explained_var: 0.018540315330028534
      vf_loss: 1.944054126739502
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0967334434390068
      entropy_coeff: 1.9946000576019287
      kl: 4.241187525622081e-06
      model: {}
      policy_loss: -3.446824848651886e-05
      total_loss: -0.1835659146308899
      vf_explained_var: 0.04709632694721222
      vf_loss: 0.18826180696487427
  load_time_ms: 362.94
  num_steps_sampled: 150000
  num_steps_trained: 150000
  sample_time_ms: 1698.297
  update_time_ms: 7.012
iterations_since_restore: 20
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.2
  gpu_util_percent0: 0.0
  ram_util_percent: 29.15
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 2.6957469379647563
  p: 11.239998120270752
policy_reward_mean:
  a: -10.437127884528422
  p: 3.617754705589627
policy_reward_min:
  a: -20.440111689901308
  p: 0.8600713285067153
sampler_perf:
  mean_env_wait_ms: 2.9721361770636237
  mean_inference_ms: 4.0009653251449
  mean_processing_ms: 1.1055614713290471
time_since_restore: 73.50756335258484
time_this_iter_s: 2.9784529209136963
time_total_s: 102.0166232585907
timestamp: 1714883574
timesteps_since_restore: 120000
timesteps_this_iter: 6000
timesteps_total: 150000
training_iteration: 25

2024-05-05 06:32:58,040 Iter 26: steps this-iter 6000 total 156000 -> 150/2000 episodes done
2024-05-05 06:33:01,106 Iter 27: steps this-iter 6000 total 162000 -> 150/2000 episodes done
2024-05-05 06:33:04,339 Iter 28: steps this-iter 6000 total 168000 -> 150/2000 episodes done
2024-05-05 06:33:07,373 Iter 29: steps this-iter 6000 total 174000 -> 150/2000 episodes done
2024-05-05 06:33:10,326 Iter 30: steps this-iter 6000 total 180000 -> 180/2000 episodes done
2024-05-05 06:33:10,328 custom_metrics: {}
date: 2024-05-05_06-33-10
done: false
episode_len_mean: 1000.0
episode_reward_max: -2.120306311056324
episode_reward_mean: -29.083525875645282
episode_reward_min: -47.11710579389468
episodes_this_iter: 30
episodes_total: 180
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 980.819
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.44160693883895874
      entropy_coeff: 0.02500000037252903
      kl: 0.0010038871550932527
      model: {}
      policy_loss: -0.0005860421806573868
      total_loss: 0.010096505284309387
      vf_explained_var: 0.023523792624473572
      vf_loss: 0.43445441126823425
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09673219174146652
      entropy_coeff: 1.9934749603271484
      kl: 3.227992010579328e-06
      model: {}
      policy_loss: -7.4427807703614235e-06
      total_loss: -0.18618270754814148
      vf_explained_var: 0.08684462308883667
      vf_loss: 0.1331590712070465
  load_time_ms: 354.505
  num_steps_sampled: 180000
  num_steps_trained: 180000
  sample_time_ms: 1701.52
  update_time_ms: 6.529
iterations_since_restore: 25
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 49.06666666666667
  gpu_util_percent0: 0.0
  ram_util_percent: 29.166666666666668
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 12.538820698266392
  p: 10.252280400785825
policy_reward_mean:
  a: -8.322822428694014
  p: 4.20776383913077
policy_reward_min:
  a: -18.845809520061266
  p: 0.6729879951735239
sampler_perf:
  mean_env_wait_ms: 2.975242925963148
  mean_inference_ms: 3.980471357968397
  mean_processing_ms: 1.1102777937607058
time_since_restore: 88.87678933143616
time_this_iter_s: 2.9510512351989746
time_total_s: 117.38584923744202
timestamp: 1714883590
timesteps_since_restore: 150000
timesteps_this_iter: 6000
timesteps_total: 180000
training_iteration: 30

2024-05-05 06:33:13,518 Iter 31: steps this-iter 6000 total 186000 -> 180/2000 episodes done
2024-05-05 06:33:16,654 Iter 32: steps this-iter 6000 total 192000 -> 180/2000 episodes done
2024-05-05 06:33:19,680 Iter 33: steps this-iter 6000 total 198000 -> 180/2000 episodes done
2024-05-05 06:33:22,918 Iter 34: steps this-iter 6000 total 204000 -> 180/2000 episodes done
2024-05-05 06:33:25,916 Iter 35: steps this-iter 6000 total 210000 -> 210/2000 episodes done
2024-05-05 06:33:25,918 custom_metrics: {}
date: 2024-05-05_06-33-25
done: false
episode_len_mean: 1000.0
episode_reward_max: -9.580362681814258
episode_reward_mean: -27.213959306903007
episode_reward_min: -49.248434876951116
episodes_this_iter: 30
episodes_total: 210
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 996.142
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.42511415481567383
      entropy_coeff: 0.02500000037252903
      kl: 0.0007089440478011966
      model: {}
      policy_loss: -0.00011713895946741104
      total_loss: 0.007782249245792627
      vf_explained_var: 0.10851588100194931
      vf_loss: 0.37054479122161865
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09675440192222595
      entropy_coeff: 1.9923499822616577
      kl: 5.448416231956799e-06
      model: {}
      policy_loss: 2.6834197342395782e-05
      total_loss: -0.1822412610054016
      vf_explained_var: 0.02251817286014557
      vf_loss: 0.21001096069812775
  load_time_ms: 350.28
  num_steps_sampled: 210000
  num_steps_trained: 210000
  sample_time_ms: 1734.086
  update_time_ms: 6.772
iterations_since_restore: 30
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.449999999999996
  gpu_util_percent0: 0.0
  ram_util_percent: 29.35
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 6.142180312624902
  p: 11.470200391582786
policy_reward_mean:
  a: -7.958684238831627
  p: 4.620777648423511
policy_reward_min:
  a: -16.57007464038228
  p: 0.592987995173811
sampler_perf:
  mean_env_wait_ms: 2.9949509964408807
  mean_inference_ms: 3.981508130941935
  mean_processing_ms: 1.1144864150618723
time_since_restore: 104.45706748962402
time_this_iter_s: 2.9966955184936523
time_total_s: 132.96612739562988
timestamp: 1714883605
timesteps_since_restore: 180000
timesteps_this_iter: 6000
timesteps_total: 210000
training_iteration: 35

2024-05-05 06:33:28,966 Iter 36: steps this-iter 6000 total 216000 -> 210/2000 episodes done
2024-05-05 06:33:32,080 Iter 37: steps this-iter 6000 total 222000 -> 210/2000 episodes done
2024-05-05 06:33:35,065 Iter 38: steps this-iter 6000 total 228000 -> 210/2000 episodes done
2024-05-05 06:33:38,321 Iter 39: steps this-iter 6000 total 234000 -> 210/2000 episodes done
2024-05-05 06:33:41,344 Iter 40: steps this-iter 6000 total 240000 -> 240/2000 episodes done
2024-05-05 06:33:41,346 custom_metrics: {}
date: 2024-05-05_06-33-41
done: false
episode_len_mean: 1000.0
episode_reward_max: 6.578821029890044
episode_reward_mean: -21.819954801368056
episode_reward_min: -46.92251316585028
episodes_this_iter: 30
episodes_total: 240
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 985.606
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.38951575756073
      entropy_coeff: 0.02500000037252903
      kl: 0.0005262470804154873
      model: {}
      policy_loss: -0.0005844561383128166
      total_loss: 0.030867457389831543
      vf_explained_var: 0.1788199245929718
      vf_loss: 0.8237960934638977
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09675652533769608
      entropy_coeff: 1.991225004196167
      kl: 6.544498774019303e-06
      model: {}
      policy_loss: 6.85080885887146e-06
      total_loss: -0.16638246178627014
      vf_explained_var: 0.0241708904504776
      vf_loss: 0.525493860244751
  load_time_ms: 357.458
  num_steps_sampled: 240000
  num_steps_trained: 240000
  sample_time_ms: 1741.614
  update_time_ms: 6.957
iterations_since_restore: 35
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.550000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 29.25
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 9.357354951778447
  p: 16.357713644573696
policy_reward_mean:
  a: -7.103427979104732
  p: 6.5937571150508685
policy_reward_min:
  a: -18.996622016198177
  p: 0.34430932367821454
sampler_perf:
  mean_env_wait_ms: 2.998270023801648
  mean_inference_ms: 3.9739111381014576
  mean_processing_ms: 1.115672508773364
time_since_restore: 119.87507963180542
time_this_iter_s: 3.021807909011841
time_total_s: 148.38413953781128
timestamp: 1714883621
timesteps_since_restore: 210000
timesteps_this_iter: 6000
timesteps_total: 240000
training_iteration: 40

2024-05-05 06:33:44,515 Iter 41: steps this-iter 6000 total 246000 -> 240/2000 episodes done
2024-05-05 06:33:47,597 Iter 42: steps this-iter 6000 total 252000 -> 240/2000 episodes done
2024-05-05 06:33:50,688 Iter 43: steps this-iter 6000 total 258000 -> 240/2000 episodes done
2024-05-05 06:33:53,696 Iter 44: steps this-iter 6000 total 264000 -> 240/2000 episodes done
2024-05-05 06:33:56,685 Iter 45: steps this-iter 6000 total 270000 -> 270/2000 episodes done
2024-05-05 06:33:56,686 custom_metrics: {}
date: 2024-05-05_06-33-56
done: false
episode_len_mean: 1000.0
episode_reward_max: 9.024524027716872
episode_reward_mean: -23.105316969180546
episode_reward_min: -51.20792088789955
episodes_this_iter: 30
episodes_total: 270
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 985.387
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3274924159049988
      entropy_coeff: 0.02500000037252903
      kl: 0.002964099869132042
      model: {}
      policy_loss: -0.0009567216038703918
      total_loss: 0.012794360518455505
      vf_explained_var: 0.25099846720695496
      vf_loss: 0.438767671585083
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09661289304494858
      entropy_coeff: 1.9901000261306763
      kl: 1.255442293768283e-05
      model: {}
      policy_loss: -5.318969488143921e-05
      total_loss: -0.17528191208839417
      vf_explained_var: 0.05013085901737213
      vf_loss: 0.34081220626831055
  load_time_ms: 357.554
  num_steps_sampled: 270000
  num_steps_trained: 270000
  sample_time_ms: 1709.803
  update_time_ms: 6.944
iterations_since_restore: 40
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.3
  gpu_util_percent0: 0.0
  ram_util_percent: 29.225
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 14.012372432280785
  p: 16.460669932584562
policy_reward_mean:
  a: -7.098234437691217
  p: 5.287620781584323
policy_reward_min:
  a: -19.32249999999906
  p: 3.1252218152510247e-12
sampler_perf:
  mean_env_wait_ms: 2.9915973384574124
  mean_inference_ms: 3.9592334341417823
  mean_processing_ms: 1.1170153845718827
time_since_restore: 135.1526482105255
time_this_iter_s: 2.986945390701294
time_total_s: 163.66170811653137
timestamp: 1714883636
timesteps_since_restore: 240000
timesteps_this_iter: 6000
timesteps_total: 270000
training_iteration: 45

2024-05-05 06:33:59,919 Iter 46: steps this-iter 6000 total 276000 -> 270/2000 episodes done
2024-05-05 06:34:02,916 Iter 47: steps this-iter 6000 total 282000 -> 270/2000 episodes done
2024-05-05 06:34:05,966 Iter 48: steps this-iter 6000 total 288000 -> 270/2000 episodes done
2024-05-05 06:34:09,043 Iter 49: steps this-iter 6000 total 294000 -> 270/2000 episodes done
2024-05-05 06:34:12,068 Iter 50: steps this-iter 6000 total 300000 -> 300/2000 episodes done
2024-05-05 06:34:12,070 custom_metrics: {}
date: 2024-05-05_06-34-12
done: false
episode_len_mean: 1000.0
episode_reward_max: 12.353372467199552
episode_reward_mean: -14.538867930112916
episode_reward_min: -43.96166916281753
episodes_this_iter: 30
episodes_total: 300
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 982.687
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3635437488555908
      entropy_coeff: 0.02500000037252903
      kl: 0.0012290155282244086
      model: {}
      policy_loss: -0.0002919258549809456
      total_loss: 0.014045109041035175
      vf_explained_var: 0.31482183933258057
      vf_loss: 0.46851256489753723
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09679219871759415
      entropy_coeff: 1.9889750480651855
      kl: 1.1047646694350988e-05
      model: {}
      policy_loss: 2.383161336183548e-05
      total_loss: -0.16978660225868225
      vf_explained_var: 0.0811181366443634
      vf_loss: 0.4541371762752533
  load_time_ms: 359.5
  num_steps_sampled: 300000
  num_steps_trained: 300000
  sample_time_ms: 1707.067
  update_time_ms: 6.452
iterations_since_restore: 45
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.550000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 29.3
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 14.825729471951853
  p: 14.437205300785385
policy_reward_mean:
  a: -5.137522227061442
  p: 6.011220978132852
policy_reward_min:
  a: -14.940609203104486
  p: 0.8284759903443161
sampler_perf:
  mean_env_wait_ms: 2.99324687532119
  mean_inference_ms: 3.9550682290794197
  mean_processing_ms: 1.1171492330189992
time_since_restore: 150.52550101280212
time_this_iter_s: 3.023721218109131
time_total_s: 179.03456091880798
timestamp: 1714883652
timesteps_since_restore: 270000
timesteps_this_iter: 6000
timesteps_total: 300000
training_iteration: 50

2024-05-05 06:34:15,314 Iter 51: steps this-iter 6000 total 306000 -> 300/2000 episodes done
2024-05-05 06:34:18,322 Iter 52: steps this-iter 6000 total 312000 -> 300/2000 episodes done
2024-05-05 06:34:21,517 Iter 53: steps this-iter 6000 total 318000 -> 300/2000 episodes done
2024-05-05 06:34:24,544 Iter 54: steps this-iter 6000 total 324000 -> 300/2000 episodes done
2024-05-05 06:34:27,814 Iter 55: steps this-iter 6000 total 330000 -> 330/2000 episodes done
2024-05-05 06:34:27,816 custom_metrics: {}
date: 2024-05-05_06-34-27
done: false
episode_len_mean: 1000.0
episode_reward_max: 41.03343276739315
episode_reward_mean: -7.8062787521211225
episode_reward_min: -41.60760619925358
episodes_this_iter: 30
episodes_total: 330
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 979.552
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4236930012702942
      entropy_coeff: 0.02500000037252903
      kl: 0.0027633816935122013
      model: {}
      policy_loss: -0.0006560198962688446
      total_loss: 0.02473290264606476
      vf_explained_var: 0.2889871597290039
      vf_loss: 0.7196247577667236
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09685955196619034
      entropy_coeff: 1.9878499507904053
      kl: 8.161202458722983e-06
      model: {}
      policy_loss: 5.7116150856018066e-05
      total_loss: -0.14804485440254211
      vf_explained_var: -0.06456577777862549
      vf_loss: 0.8888051509857178
  load_time_ms: 361.1
  num_steps_sampled: 330000
  num_steps_trained: 330000
  sample_time_ms: 1748.56
  update_time_ms: 6.494
iterations_since_restore: 50
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 44.724999999999994
  gpu_util_percent0: 0.0
  ram_util_percent: 29.25
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 11.282561740385532
  p: 27.23496621979229
policy_reward_mean:
  a: -4.026194677868438
  p: 8.29849995935263
policy_reward_min:
  a: -13.791248114621858
  p: 0.24250000000294053
sampler_perf:
  mean_env_wait_ms: 3.008260475183867
  mean_inference_ms: 3.9619965898638205
  mean_processing_ms: 1.120922518814301
time_since_restore: 166.20603036880493
time_this_iter_s: 3.2682642936706543
time_total_s: 194.7150902748108
timestamp: 1714883667
timesteps_since_restore: 300000
timesteps_this_iter: 6000
timesteps_total: 330000
training_iteration: 55

2024-05-05 06:34:30,906 Iter 56: steps this-iter 6000 total 336000 -> 330/2000 episodes done
2024-05-05 06:34:34,053 Iter 57: steps this-iter 6000 total 342000 -> 330/2000 episodes done
2024-05-05 06:34:37,275 Iter 58: steps this-iter 6000 total 348000 -> 330/2000 episodes done
2024-05-05 06:34:40,247 Iter 59: steps this-iter 6000 total 354000 -> 330/2000 episodes done
2024-05-05 06:34:43,367 Iter 60: steps this-iter 6000 total 360000 -> 360/2000 episodes done
2024-05-05 06:34:43,369 custom_metrics: {}
date: 2024-05-05_06-34-43
done: false
episode_len_mean: 1000.0
episode_reward_max: 22.86419945094504
episode_reward_mean: -11.375423378744038
episode_reward_min: -40.27402821346494
episodes_this_iter: 30
episodes_total: 360
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 985.72
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3190961182117462
      entropy_coeff: 0.02500000037252903
      kl: 0.0007565808482468128
      model: {}
      policy_loss: 0.00018794089555740356
      total_loss: 0.02124873176217079
      vf_explained_var: 0.2828119993209839
      vf_loss: 0.5807639360427856
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09681303799152374
      entropy_coeff: 1.9867249727249146
      kl: 5.653541848005261e-06
      model: {}
      policy_loss: -3.4458935260772705e-08
      total_loss: -0.13527636229991913
      vf_explained_var: 0.025385111570358276
      vf_loss: 1.1412910223007202
  load_time_ms: 353.045
  num_steps_sampled: 360000
  num_steps_trained: 360000
  sample_time_ms: 1759.341
  update_time_ms: 7.022
iterations_since_restore: 55
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.75
  gpu_util_percent0: 0.0
  ram_util_percent: 29.075
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 10.018947455305593
  p: 22.870829354619044
policy_reward_mean:
  a: -4.735477454878954
  p: 7.566486440771777
policy_reward_min:
  a: -12.80565368143639
  p: 0.4780593236780984
sampler_perf:
  mean_env_wait_ms: 3.008022780231002
  mean_inference_ms: 3.957241591376196
  mean_processing_ms: 1.1224734293014118
time_since_restore: 181.69663667678833
time_this_iter_s: 3.118932008743286
time_total_s: 210.2056965827942
timestamp: 1714883683
timesteps_since_restore: 330000
timesteps_this_iter: 6000
timesteps_total: 360000
training_iteration: 60

2024-05-05 06:34:46,362 Iter 61: steps this-iter 6000 total 366000 -> 360/2000 episodes done
2024-05-05 06:34:49,593 Iter 62: steps this-iter 6000 total 372000 -> 360/2000 episodes done
2024-05-05 06:34:52,800 Iter 63: steps this-iter 6000 total 378000 -> 360/2000 episodes done
2024-05-05 06:34:56,173 Iter 64: steps this-iter 6000 total 384000 -> 360/2000 episodes done
2024-05-05 06:34:59,335 Iter 65: steps this-iter 6000 total 390000 -> 390/2000 episodes done
2024-05-05 06:34:59,337 custom_metrics: {}
date: 2024-05-05_06-34-59
done: false
episode_len_mean: 1000.0
episode_reward_max: 8.00019432281133
episode_reward_mean: -17.761596690443913
episode_reward_min: -36.720518786667874
episodes_this_iter: 30
episodes_total: 390
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 988.592
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3483304977416992
      entropy_coeff: 0.02500000037252903
      kl: 0.00035556472721509635
      model: {}
      policy_loss: -0.00025146990083158016
      total_loss: 0.006999610923230648
      vf_explained_var: 0.19349513947963715
      vf_loss: 0.31918686628341675
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09687912464141846
      entropy_coeff: 1.9855999946594238
      kl: 2.816561618601554e-06
      model: {}
      policy_loss: 2.7399510145187378e-05
      total_loss: -0.172579824924469
      vf_explained_var: 0.12282021343708038
      vf_loss: 0.3951197564601898
  load_time_ms: 351.359
  num_steps_sampled: 390000
  num_steps_trained: 390000
  sample_time_ms: 1779.802
  update_time_ms: 7.96
iterations_since_restore: 60
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 42.875
  gpu_util_percent0: 0.0
  ram_util_percent: 29.3
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 11.424158184695727
  p: 14.468962501006832
policy_reward_mean:
  a: -5.774018502348747
  p: 5.334477318951073
policy_reward_min:
  a: -14.286808263024334
  p: 0.6784759903446886
sampler_perf:
  mean_env_wait_ms: 3.010585471444292
  mean_inference_ms: 3.9746311874226476
  mean_processing_ms: 1.1243124039700318
time_since_restore: 197.58622074127197
time_this_iter_s: 3.117401361465454
time_total_s: 226.09528064727783
timestamp: 1714883699
timesteps_since_restore: 360000
timesteps_this_iter: 6000
timesteps_total: 390000
training_iteration: 65

2024-05-05 06:35:02,310 Iter 66: steps this-iter 6000 total 396000 -> 390/2000 episodes done
2024-05-05 06:35:05,473 Iter 67: steps this-iter 6000 total 402000 -> 390/2000 episodes done
2024-05-05 06:35:08,469 Iter 68: steps this-iter 6000 total 408000 -> 390/2000 episodes done
2024-05-05 06:35:11,474 Iter 69: steps this-iter 6000 total 414000 -> 390/2000 episodes done
2024-05-05 06:35:14,703 Iter 70: steps this-iter 6000 total 420000 -> 420/2000 episodes done
2024-05-05 06:35:14,705 custom_metrics: {}
date: 2024-05-05_06-35-14
done: false
episode_len_mean: 1000.0
episode_reward_max: 28.024010955506863
episode_reward_mean: -15.412639071506122
episode_reward_min: -39.4467188587621
episodes_this_iter: 30
episodes_total: 420
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 982.526
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.38642120361328125
      entropy_coeff: 0.02500000037252903
      kl: 0.0004377711738925427
      model: {}
      policy_loss: -0.0002883598208427429
      total_loss: 0.020488670095801353
      vf_explained_var: 0.19967520236968994
      vf_loss: 0.6087511777877808
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09684225916862488
      entropy_coeff: 1.984475016593933
      kl: 2.914185643021483e-06
      model: {}
      policy_loss: -2.0417850464582443e-05
      total_loss: -0.15168678760528564
      vf_explained_var: 0.0823732316493988
      vf_loss: 0.8102933764457703
  load_time_ms: 355.318
  num_steps_sampled: 420000
  num_steps_trained: 420000
  sample_time_ms: 1771.353
  update_time_ms: 7.44
iterations_since_restore: 65
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 44.15
  gpu_util_percent0: 0.0
  ram_util_percent: 29.375
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.385306263355403
  p: 22.93078699312325
policy_reward_mean:
  a: -5.532926958694715
  p: 6.71906876327274
policy_reward_min:
  a: -12.7322496226984
  p: 0.46333333333628085
sampler_perf:
  mean_env_wait_ms: 3.0083351134275857
  mean_inference_ms: 3.9683231912423222
  mean_processing_ms: 1.1264519798686266
time_since_restore: 212.94354009628296
time_this_iter_s: 3.2269985675811768
time_total_s: 241.45260000228882
timestamp: 1714883714
timesteps_since_restore: 390000
timesteps_this_iter: 6000
timesteps_total: 420000
training_iteration: 70

2024-05-05 06:35:17,698 Iter 71: steps this-iter 6000 total 426000 -> 420/2000 episodes done
2024-05-05 06:35:20,957 Iter 72: steps this-iter 6000 total 432000 -> 420/2000 episodes done
2024-05-05 06:35:23,917 Iter 73: steps this-iter 6000 total 438000 -> 420/2000 episodes done
2024-05-05 06:35:26,943 Iter 74: steps this-iter 6000 total 444000 -> 420/2000 episodes done
2024-05-05 06:35:29,977 Iter 75: steps this-iter 6000 total 450000 -> 450/2000 episodes done
2024-05-05 06:35:29,979 custom_metrics: {}
date: 2024-05-05_06-35-29
done: false
episode_len_mean: 1000.0
episode_reward_max: 17.89331021320806
episode_reward_mean: -13.51705485544411
episode_reward_min: -36.79728085171153
episodes_this_iter: 30
episodes_total: 450
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 969.83
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.30398544669151306
      entropy_coeff: 0.02500000037252903
      kl: 0.00040940524195320904
      model: {}
      policy_loss: 3.375718370079994e-05
      total_loss: 0.014970941469073296
      vf_explained_var: 0.23399779200553894
      vf_loss: 0.450736403465271
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09687648713588715
      entropy_coeff: 1.9833500385284424
      kl: 4.231158527545631e-06
      model: {}
      policy_loss: -2.035871148109436e-06
      total_loss: -0.17079496383666992
      vf_explained_var: 0.32050850987434387
      vf_loss: 0.42694091796875
  load_time_ms: 360.15
  num_steps_sampled: 450000
  num_steps_trained: 450000
  sample_time_ms: 1714.716
  update_time_ms: 6.273
iterations_since_restore: 70
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 50.300000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 29.099999999999998
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 10.713040376309486
  p: 18.39421998714427
policy_reward_mean:
  a: -4.992414945361039
  p: 6.4526049260000455
policy_reward_min:
  a: -14.447168788247268
  p: 0.544309323678072
sampler_perf:
  mean_env_wait_ms: 3.006131570174536
  mean_inference_ms: 3.9621132730152024
  mean_processing_ms: 1.125487853716553
time_since_restore: 228.17676258087158
time_this_iter_s: 3.0305635929107666
time_total_s: 256.68582248687744
timestamp: 1714883729
timesteps_since_restore: 420000
timesteps_this_iter: 6000
timesteps_total: 450000
training_iteration: 75

2024-05-05 06:35:33,064 Iter 76: steps this-iter 6000 total 456000 -> 450/2000 episodes done
2024-05-05 06:35:36,058 Iter 77: steps this-iter 6000 total 462000 -> 450/2000 episodes done
2024-05-05 06:35:39,058 Iter 78: steps this-iter 6000 total 468000 -> 450/2000 episodes done
2024-05-05 06:35:42,128 Iter 79: steps this-iter 6000 total 474000 -> 450/2000 episodes done
2024-05-05 06:35:45,107 Iter 80: steps this-iter 6000 total 480000 -> 480/2000 episodes done
2024-05-05 06:35:45,109 custom_metrics: {}
date: 2024-05-05_06-35-45
done: false
episode_len_mean: 1000.0
episode_reward_max: -0.6148959176175222
episode_reward_mean: -22.374563237329358
episode_reward_min: -37.611675259116765
episodes_this_iter: 30
episodes_total: 480
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 961.503
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.2745839059352875
      entropy_coeff: 0.02500000037252903
      kl: 0.00022408508812077343
      model: {}
      policy_loss: -0.00014502927660942078
      total_loss: 0.01896136999130249
      vf_explained_var: 0.28480133414268494
      vf_loss: 0.5194199085235596
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09693779051303864
      entropy_coeff: 1.982224941253662
      kl: 4.02607474825345e-06
      model: {}
      policy_loss: 3.9108097553253174e-05
      total_loss: -0.1730932593345642
      vf_explained_var: 0.1104927808046341
      vf_loss: 0.3804026246070862
  load_time_ms: 365.84
  num_steps_sampled: 480000
  num_steps_trained: 480000
  sample_time_ms: 1693.533
  update_time_ms: 6.162
iterations_since_restore: 75
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.075
  gpu_util_percent0: 0.0
  ram_util_percent: 29.275
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.535714470072897
  p: 12.862739075069557
policy_reward_mean:
  a: -6.588449384717552
  p: 3.9792343015408482
policy_reward_min:
  a: -13.956761459774498
  p: 0.4091666666696361
sampler_perf:
  mean_env_wait_ms: 2.9992339478766468
  mean_inference_ms: 3.9524220329993143
  mean_processing_ms: 1.124848711902008
time_since_restore: 243.29558658599854
time_this_iter_s: 2.9770052433013916
time_total_s: 271.8046464920044
timestamp: 1714883745
timesteps_since_restore: 450000
timesteps_this_iter: 6000
timesteps_total: 480000
training_iteration: 80

2024-05-05 06:35:48,145 Iter 81: steps this-iter 6000 total 486000 -> 480/2000 episodes done
2024-05-05 06:35:51,163 Iter 82: steps this-iter 6000 total 492000 -> 480/2000 episodes done
2024-05-05 06:35:54,253 Iter 83: steps this-iter 6000 total 498000 -> 480/2000 episodes done
2024-05-05 06:35:57,259 Iter 84: steps this-iter 6000 total 504000 -> 480/2000 episodes done
2024-05-05 06:36:00,427 Iter 85: steps this-iter 6000 total 510000 -> 510/2000 episodes done
2024-05-05 06:36:00,429 custom_metrics: {}
date: 2024-05-05_06-36-00
done: false
episode_len_mean: 1000.0
episode_reward_max: 3.6986692079357955
episode_reward_mean: -21.869434883991282
episode_reward_min: -40.48167454851728
episodes_this_iter: 30
episodes_total: 510
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 963.806
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.28806740045547485
      entropy_coeff: 0.02500000037252903
      kl: 0.0006072294199839234
      model: {}
      policy_loss: -1.3149343430995941e-05
      total_loss: 0.015334270894527435
      vf_explained_var: 0.2522168755531311
      vf_loss: 0.45098212361335754
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09686867892742157
      entropy_coeff: 1.9810999631881714
      kl: 4.141055796935689e-06
      model: {}
      policy_loss: 7.040798664093018e-07
      total_loss: -0.17441576719284058
      vf_explained_var: 0.20043018460273743
      vf_loss: 0.3498013913631439
  load_time_ms: 363.714
  num_steps_sampled: 510000
  num_steps_trained: 510000
  sample_time_ms: 1700.45
  update_time_ms: 6.508
iterations_since_restore: 80
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.15
  gpu_util_percent0: 0.0
  ram_util_percent: 29.525
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 8.663070491964497
  p: 12.880570720390413
policy_reward_mean:
  a: -6.4285695730049035
  p: 3.8448434080283276
policy_reward_min:
  a: -15.14700872604254
  p: 0.19340466184062885
sampler_perf:
  mean_env_wait_ms: 2.9995627719442153
  mean_inference_ms: 3.9489736815873497
  mean_processing_ms: 1.1261497485479477
time_since_restore: 258.60323190689087
time_this_iter_s: 3.1659719944000244
time_total_s: 287.11229181289673
timestamp: 1714883760
timesteps_since_restore: 480000
timesteps_this_iter: 6000
timesteps_total: 510000
training_iteration: 85

2024-05-05 06:36:03,430 Iter 86: steps this-iter 6000 total 516000 -> 510/2000 episodes done
2024-05-05 06:36:06,440 Iter 87: steps this-iter 6000 total 522000 -> 510/2000 episodes done
2024-05-05 06:36:09,643 Iter 88: steps this-iter 6000 total 528000 -> 510/2000 episodes done
2024-05-05 06:36:12,655 Iter 89: steps this-iter 6000 total 534000 -> 510/2000 episodes done
2024-05-05 06:36:15,942 Iter 90: steps this-iter 6000 total 540000 -> 540/2000 episodes done
2024-05-05 06:36:15,943 custom_metrics: {}
date: 2024-05-05_06-36-15
done: false
episode_len_mean: 1000.0
episode_reward_max: -0.5066392329586442
episode_reward_mean: -21.04965036166074
episode_reward_min: -36.10487609257535
episodes_this_iter: 30
episodes_total: 540
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 977.425
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3304429054260254
      entropy_coeff: 0.02500000037252903
      kl: 0.0009807205060496926
      model: {}
      policy_loss: -0.00029960647225379944
      total_loss: 0.004723964724689722
      vf_explained_var: 0.15374864637851715
      vf_loss: 0.26569294929504395
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09693881869316101
      entropy_coeff: 1.9799749851226807
      kl: 2.5830768208834343e-06
      model: {}
      policy_loss: -4.179868847131729e-05
      total_loss: -0.18122565746307373
      vf_explained_var: 0.17660751938819885
      vf_loss: 0.21505168080329895
  load_time_ms: 352.712
  num_steps_sampled: 540000
  num_steps_trained: 540000
  sample_time_ms: 1731.163
  update_time_ms: 10.101
iterations_since_restore: 85
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 38.68
  gpu_util_percent0: 0.0
  ram_util_percent: 29.259999999999998
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 18.810395155122677
  p: 11.874930768878764
policy_reward_mean:
  a: -6.100300573225443
  p: 3.3515519312410236
policy_reward_min:
  a: -14.108452819494659
  p: 0.16666666666972818
sampler_perf:
  mean_env_wait_ms: 3.0018707356018113
  mean_inference_ms: 3.9491266161194623
  mean_processing_ms: 1.1272417817202638
time_since_restore: 274.10047721862793
time_this_iter_s: 3.282501697540283
time_total_s: 302.6095371246338
timestamp: 1714883775
timesteps_since_restore: 510000
timesteps_this_iter: 6000
timesteps_total: 540000
training_iteration: 90

2024-05-05 06:36:18,965 Iter 91: steps this-iter 6000 total 546000 -> 540/2000 episodes done
2024-05-05 06:36:22,117 Iter 92: steps this-iter 6000 total 552000 -> 540/2000 episodes done
2024-05-05 06:36:25,206 Iter 93: steps this-iter 6000 total 558000 -> 540/2000 episodes done
2024-05-05 06:36:28,233 Iter 94: steps this-iter 6000 total 564000 -> 540/2000 episodes done
2024-05-05 06:36:31,443 Iter 95: steps this-iter 6000 total 570000 -> 570/2000 episodes done
2024-05-05 06:36:31,445 custom_metrics: {}
date: 2024-05-05_06-36-31
done: false
episode_len_mean: 1000.0
episode_reward_max: -7.286151581387978
episode_reward_mean: -22.510555577871745
episode_reward_min: -34.359328354614064
episodes_this_iter: 30
episodes_total: 570
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 985.256
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.33837342262268066
      entropy_coeff: 0.02500000037252903
      kl: 0.0015616354066878557
      model: {}
      policy_loss: -0.00022480543702840805
      total_loss: 0.006504077930003405
      vf_explained_var: 0.17183485627174377
      vf_loss: 0.3037644326686859
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09691330790519714
      entropy_coeff: 1.97885000705719
      kl: 6.094715899962466e-06
      model: {}
      policy_loss: 4.152767360210419e-05
      total_loss: -0.18728923797607422
      vf_explained_var: 0.28057587146759033
      vf_loss: 0.0889226645231247
  load_time_ms: 353.63
  num_steps_sampled: 570000
  num_steps_trained: 570000
  sample_time_ms: 1740.995
  update_time_ms: 9.789
iterations_since_restore: 90
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 44.775
  gpu_util_percent0: 0.0
  ram_util_percent: 29.275
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 9.08989385130846
  p: 9.325640126480293
policy_reward_mean:
  a: -6.409372659279736
  p: 3.1269350592471934
policy_reward_min:
  a: -13.88980275882954
  p: 0.34583333333632854
sampler_perf:
  mean_env_wait_ms: 3.0051443715131985
  mean_inference_ms: 3.948195181102565
  mean_processing_ms: 1.1276102179944807
time_since_restore: 289.5905559062958
time_this_iter_s: 3.2080790996551514
time_total_s: 318.09961581230164
timestamp: 1714883791
timesteps_since_restore: 540000
timesteps_this_iter: 6000
timesteps_total: 570000
training_iteration: 95

2024-05-05 06:36:34,502 Iter 96: steps this-iter 6000 total 576000 -> 570/2000 episodes done
2024-05-05 06:36:37,470 Iter 97: steps this-iter 6000 total 582000 -> 570/2000 episodes done
2024-05-05 06:36:40,663 Iter 98: steps this-iter 6000 total 588000 -> 570/2000 episodes done
2024-05-05 06:36:43,717 Iter 99: steps this-iter 6000 total 594000 -> 570/2000 episodes done
2024-05-05 06:36:46,700 Iter 100: steps this-iter 6000 total 600000 -> 600/2000 episodes done
2024-05-05 06:36:46,702 custom_metrics: {}
date: 2024-05-05_06-36-46
done: false
episode_len_mean: 1000.0
episode_reward_max: 22.70431066841758
episode_reward_mean: -16.592262189433146
episode_reward_min: -42.65109427176316
episodes_this_iter: 30
episodes_total: 600
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 980.837
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.31907546520233154
      entropy_coeff: 0.02500000037252903
      kl: 0.0008059662650339305
      model: {}
      policy_loss: 5.9252604842185974e-05
      total_loss: 0.020102044567465782
      vf_explained_var: 0.18260355293750763
      vf_loss: 0.5603936910629272
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0968741625547409
      entropy_coeff: 1.9777250289916992
      kl: 7.0705623329558875e-06
      model: {}
      policy_loss: -2.8977170586586e-05
      total_loss: -0.16342036426067352
      vf_explained_var: 0.18983447551727295
      vf_loss: 0.5639816522598267
  load_time_ms: 361.04
  num_steps_sampled: 600000
  num_steps_trained: 600000
  sample_time_ms: 1716.882
  update_time_ms: 6.626
iterations_since_restore: 95
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 50.36666666666667
  gpu_util_percent0: 0.0
  ram_util_percent: 29.2
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 8.516893364514267
  p: 18.976499299985928
policy_reward_mean:
  a: -5.56242952745522
  p: 5.657455920387733
policy_reward_min:
  a: -14.574381539392352
  p: 0.2675000000028306
sampler_perf:
  mean_env_wait_ms: 3.001130013069123
  mean_inference_ms: 3.9420146046066598
  mean_processing_ms: 1.1283738703932085
time_since_restore: 304.8362526893616
time_this_iter_s: 2.981117010116577
time_total_s: 333.34531259536743
timestamp: 1714883806
timesteps_since_restore: 570000
timesteps_this_iter: 6000
timesteps_total: 600000
training_iteration: 100

2024-05-05 06:36:46,837 >> Wrote dense logs to: ../../../runs/phase2/dense_logs/logs_0000000000600000
2024-05-05 06:36:50,114 Iter 101: steps this-iter 6000 total 606000 -> 600/2000 episodes done
2024-05-05 06:36:53,244 Iter 102: steps this-iter 6000 total 612000 -> 600/2000 episodes done
2024-05-05 06:36:56,299 Iter 103: steps this-iter 6000 total 618000 -> 600/2000 episodes done
2024-05-05 06:36:59,519 Iter 104: steps this-iter 6000 total 624000 -> 600/2000 episodes done
2024-05-05 06:37:02,852 Iter 105: steps this-iter 6000 total 630000 -> 630/2000 episodes done
2024-05-05 06:37:02,853 custom_metrics: {}
date: 2024-05-05_06-37-02
done: false
episode_len_mean: 1000.0
episode_reward_max: 4.05045731415516
episode_reward_mean: -21.980420990850874
episode_reward_min: -41.37727840547529
episodes_this_iter: 30
episodes_total: 630
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 993.994
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3475329577922821
      entropy_coeff: 0.02500000037252903
      kl: 0.00031032488914206624
      model: {}
      policy_loss: 4.8796646296978e-05
      total_loss: 0.012437839061021805
      vf_explained_var: 0.1629175990819931
      vf_loss: 0.4215473532676697
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09690465778112411
      entropy_coeff: 1.9766000509262085
      kl: 4.137450559937861e-06
      model: {}
      policy_loss: 3.1871721148490906e-05
      total_loss: -0.18331336975097656
      vf_explained_var: 0.3835482597351074
      vf_loss: 0.16393011808395386
  load_time_ms: 361.418
  num_steps_sampled: 630000
  num_steps_trained: 630000
  sample_time_ms: 1754.387
  update_time_ms: 6.859
iterations_since_restore: 100
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.175
  gpu_util_percent0: 0.0
  ram_util_percent: 29.75
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 12.4986540543035
  p: 9.717465819591236
policy_reward_mean:
  a: -6.359204176535695
  p: 3.4563957152918974
policy_reward_min:
  a: -13.572396975809482
  p: 0.12125000000303274
sampler_perf:
  mean_env_wait_ms: 3.0263065634204045
  mean_inference_ms: 3.9392513106163847
  mean_processing_ms: 1.1264121320059588
time_since_restore: 320.8393876552582
time_this_iter_s: 3.33040189743042
time_total_s: 349.34844756126404
timestamp: 1714883822
timesteps_since_restore: 600000
timesteps_this_iter: 6000
timesteps_total: 630000
training_iteration: 105

2024-05-05 06:37:06,044 Iter 106: steps this-iter 6000 total 636000 -> 630/2000 episodes done
2024-05-05 06:37:09,183 Iter 107: steps this-iter 6000 total 642000 -> 630/2000 episodes done
2024-05-05 06:37:12,272 Iter 108: steps this-iter 6000 total 648000 -> 630/2000 episodes done
2024-05-05 06:37:15,325 Iter 109: steps this-iter 6000 total 654000 -> 630/2000 episodes done
2024-05-05 06:37:18,305 Iter 110: steps this-iter 6000 total 660000 -> 660/2000 episodes done
2024-05-05 06:37:18,307 custom_metrics: {}
date: 2024-05-05_06-37-18
done: false
episode_len_mean: 1000.0
episode_reward_max: -4.073611276357234
episode_reward_mean: -19.375646634555395
episode_reward_min: -32.965858223172575
episodes_this_iter: 30
episodes_total: 660
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 996.939
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.43705257773399353
      entropy_coeff: 0.02500000037252903
      kl: 0.0030723903328180313
      model: {}
      policy_loss: -7.340870797634125e-05
      total_loss: 0.014769420959055424
      vf_explained_var: 0.05918912962079048
      vf_loss: 0.5153828263282776
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09688055515289307
      entropy_coeff: 1.9754749536514282
      kl: 2.5170652406814042e-06
      model: {}
      policy_loss: -7.418915629386902e-06
      total_loss: -0.1734389364719391
      vf_explained_var: 0.20078743994235992
      vf_loss: 0.3590719997882843
  load_time_ms: 360.174
  num_steps_sampled: 660000
  num_steps_trained: 660000
  sample_time_ms: 1772.411
  update_time_ms: 6.806
iterations_since_restore: 105
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 49.4
  gpu_util_percent0: 0.0
  ram_util_percent: 29.46666666666667
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 14.859863474093341
  p: 10.43105928795423
policy_reward_mean:
  a: -5.973002738060632
  p: 4.516364317687129
policy_reward_min:
  a: -17.39547409977922
  p: 0.3684046618405617
sampler_perf:
  mean_env_wait_ms: 3.025005127992612
  mean_inference_ms: 3.937369567724326
  mean_processing_ms: 1.1265321343485828
time_since_restore: 336.2810652256012
time_this_iter_s: 2.9782111644744873
time_total_s: 364.79012513160706
timestamp: 1714883838
timesteps_since_restore: 630000
timesteps_this_iter: 6000
timesteps_total: 660000
training_iteration: 110

2024-05-05 06:37:21,441 Iter 111: steps this-iter 6000 total 666000 -> 660/2000 episodes done
2024-05-05 06:37:24,441 Iter 112: steps this-iter 6000 total 672000 -> 660/2000 episodes done
2024-05-05 06:37:27,635 Iter 113: steps this-iter 6000 total 678000 -> 660/2000 episodes done
2024-05-05 06:37:30,679 Iter 114: steps this-iter 6000 total 684000 -> 660/2000 episodes done
2024-05-05 06:37:33,778 Iter 115: steps this-iter 6000 total 690000 -> 690/2000 episodes done
2024-05-05 06:37:33,781 custom_metrics: {}
date: 2024-05-05_06-37-33
done: false
episode_len_mean: 1000.0
episode_reward_max: 16.342158905647704
episode_reward_mean: -17.889573759393844
episode_reward_min: -39.17745746748237
episodes_this_iter: 30
episodes_total: 690
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 979.672
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.32732465863227844
      entropy_coeff: 0.02500000037252903
      kl: 0.0006165566737763584
      model: {}
      policy_loss: 1.3669021427631378e-05
      total_loss: 0.014622228220105171
      vf_explained_var: 0.13088443875312805
      vf_loss: 0.45583343505859375
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09690196812152863
      entropy_coeff: 1.9743499755859375
      kl: 5.334914476406993e-06
      model: {}
      policy_loss: 2.339482307434082e-06
      total_loss: -0.16880904138088226
      vf_explained_var: 0.19239747524261475
      vf_loss: 0.45014044642448425
  load_time_ms: 359.393
  num_steps_sampled: 690000
  num_steps_trained: 690000
  sample_time_ms: 1734.943
  update_time_ms: 6.466
iterations_since_restore: 110
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 46.7
  gpu_util_percent0: 0.0
  ram_util_percent: 29.775
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.816255022892733
  p: 19.833039668513642
policy_reward_mean:
  a: -5.681812428488703
  p: 4.837675954560971
policy_reward_min:
  a: -16.035214710544114
  p: 0.41074869784906914
sampler_perf:
  mean_env_wait_ms: 3.0260359180924885
  mean_inference_ms: 3.9381011363952583
  mean_processing_ms: 1.1273749609429744
time_since_restore: 351.7263226509094
time_this_iter_s: 3.0969417095184326
time_total_s: 380.2353825569153
timestamp: 1714883853
timesteps_since_restore: 660000
timesteps_this_iter: 6000
timesteps_total: 690000
training_iteration: 115

2024-05-05 06:37:37,076 Iter 116: steps this-iter 6000 total 696000 -> 690/2000 episodes done
2024-05-05 06:37:40,112 Iter 117: steps this-iter 6000 total 702000 -> 690/2000 episodes done
2024-05-05 06:37:43,369 Iter 118: steps this-iter 6000 total 708000 -> 690/2000 episodes done
2024-05-05 06:37:46,522 Iter 119: steps this-iter 6000 total 714000 -> 690/2000 episodes done
2024-05-05 06:37:49,576 Iter 120: steps this-iter 6000 total 720000 -> 720/2000 episodes done
2024-05-05 06:37:49,578 custom_metrics: {}
date: 2024-05-05_06-37-49
done: false
episode_len_mean: 1000.0
episode_reward_max: 6.163720900625346
episode_reward_mean: -17.352082571169277
episode_reward_min: -34.290988018051685
episodes_this_iter: 30
episodes_total: 720
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 987.365
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3459088206291199
      entropy_coeff: 0.02500000037252903
      kl: 0.0008100831182673573
      model: {}
      policy_loss: -0.00036746077239513397
      total_loss: 0.0169263556599617
      vf_explained_var: 0.17575311660766602
      vf_loss: 0.5188307762145996
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0968681126832962
      entropy_coeff: 1.9732249975204468
      kl: 1.6305500594171463e-06
      model: {}
      policy_loss: -3.6264071241021156e-05
      total_loss: -0.1793670356273651
      vf_explained_var: 0.27158981561660767
      vf_loss: 0.23623648285865784
  load_time_ms: 364.307
  num_steps_sampled: 720000
  num_steps_trained: 720000
  sample_time_ms: 1756.299
  update_time_ms: 6.31
iterations_since_restore: 115
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.875
  gpu_util_percent0: 0.0
  ram_util_percent: 29.9
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 18.559256734314488
  p: 11.401930026426275
policy_reward_mean:
  a: -5.307398234670427
  p: 3.8775103675124236
policy_reward_min:
  a: -13.09465868516321
  p: 0.3271546618406155
sampler_perf:
  mean_env_wait_ms: 3.029844593421644
  mean_inference_ms: 3.9411073186896033
  mean_processing_ms: 1.1281637116283434
time_since_restore: 367.50865483283997
time_this_iter_s: 3.0509934425354004
time_total_s: 396.0177147388458
timestamp: 1714883869
timesteps_since_restore: 690000
timesteps_this_iter: 6000
timesteps_total: 720000
training_iteration: 120

2024-05-05 06:37:52,837 Iter 121: steps this-iter 6000 total 726000 -> 720/2000 episodes done
2024-05-05 06:37:55,970 Iter 122: steps this-iter 6000 total 732000 -> 720/2000 episodes done
2024-05-05 06:37:59,135 Iter 123: steps this-iter 6000 total 738000 -> 720/2000 episodes done
2024-05-05 06:38:02,337 Iter 124: steps this-iter 6000 total 744000 -> 720/2000 episodes done
2024-05-05 06:38:05,373 Iter 125: steps this-iter 6000 total 750000 -> 750/2000 episodes done
2024-05-05 06:38:05,375 custom_metrics: {}
date: 2024-05-05_06-38-05
done: false
episode_len_mean: 1000.0
episode_reward_max: -2.869079547087761
episode_reward_mean: -21.965920405598865
episode_reward_min: -37.36915447649999
episodes_this_iter: 30
episodes_total: 750
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1005.037
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.293895959854126
      entropy_coeff: 0.02500000037252903
      kl: 0.00032361215562559664
      model: {}
      policy_loss: -3.718794323503971e-05
      total_loss: 0.033394381403923035
      vf_explained_var: 0.1354205459356308
      vf_loss: 0.815579354763031
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09690040349960327
      entropy_coeff: 1.972100019454956
      kl: 1.7938923519977834e-06
      model: {}
      policy_loss: -1.7222482711076736e-05
      total_loss: -0.17795513570308685
      vf_explained_var: 0.018051758408546448
      vf_loss: 0.263187438249588
  load_time_ms: 370.512
  num_steps_sampled: 750000
  num_steps_trained: 750000
  sample_time_ms: 1766.638
  update_time_ms: 6.265
iterations_since_restore: 120
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.225
  gpu_util_percent0: 0.0
  ram_util_percent: 30.049999999999997
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 14.442549097195531
  p: 9.616427395093186
policy_reward_mean:
  a: -6.321448362876821
  p: 3.3198730459084134
policy_reward_min:
  a: -13.280417758092227
  p: 0.33333333333633186
sampler_perf:
  mean_env_wait_ms: 3.032495800562042
  mean_inference_ms: 3.942300610338127
  mean_processing_ms: 1.1307322968874132
time_since_restore: 383.294495344162
time_this_iter_s: 3.0338408946990967
time_total_s: 411.80355525016785
timestamp: 1714883885
timesteps_since_restore: 720000
timesteps_this_iter: 6000
timesteps_total: 750000
training_iteration: 125

2024-05-05 06:38:08,494 Iter 126: steps this-iter 6000 total 756000 -> 750/2000 episodes done
2024-05-05 06:38:11,555 Iter 127: steps this-iter 6000 total 762000 -> 750/2000 episodes done
2024-05-05 06:38:14,748 Iter 128: steps this-iter 6000 total 768000 -> 750/2000 episodes done
2024-05-05 06:38:17,878 Iter 129: steps this-iter 6000 total 774000 -> 750/2000 episodes done
2024-05-05 06:38:20,904 Iter 130: steps this-iter 6000 total 780000 -> 780/2000 episodes done
2024-05-05 06:38:20,905 custom_metrics: {}
date: 2024-05-05_06-38-20
done: false
episode_len_mean: 1000.0
episode_reward_max: 38.28665100176961
episode_reward_mean: -19.326577345128502
episode_reward_min: -35.266392398791716
episodes_this_iter: 30
episodes_total: 780
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1001.75
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.28961116075515747
      entropy_coeff: 0.02500000037252903
      kl: 0.0014239831361919641
      model: {}
      policy_loss: -0.0004614470526576042
      total_loss: 0.019263675436377525
      vf_explained_var: 0.2693250775337219
      vf_loss: 0.5393079519271851
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09696230292320251
      entropy_coeff: 1.9709750413894653
      kl: 2.2076501409173943e-06
      model: {}
      policy_loss: -2.9802322387695312e-08
      total_loss: -0.17662197351455688
      vf_explained_var: 0.10595686733722687
      vf_loss: 0.28976696729660034
  load_time_ms: 370.868
  num_steps_sampled: 780000
  num_steps_trained: 780000
  sample_time_ms: 1743.276
  update_time_ms: 6.35
iterations_since_restore: 125
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.625
  gpu_util_percent0: 0.0
  ram_util_percent: 29.900000000000002
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 12.2765910033999
  p: 24.835134786920452
policy_reward_mean:
  a: -5.846631076375854
  p: 4.059946960374914
policy_reward_min:
  a: -13.246945849740428
  p: 0.24250000000303332
sampler_perf:
  mean_env_wait_ms: 3.03246839306217
  mean_inference_ms: 3.9406342739922398
  mean_processing_ms: 1.1315811704435714
time_since_restore: 398.81289625167847
time_this_iter_s: 3.0239665508270264
time_total_s: 427.3219561576843
timestamp: 1714883900
timesteps_since_restore: 750000
timesteps_this_iter: 6000
timesteps_total: 780000
training_iteration: 130

2024-05-05 06:38:24,165 Iter 131: steps this-iter 6000 total 786000 -> 780/2000 episodes done
2024-05-05 06:38:27,227 Iter 132: steps this-iter 6000 total 792000 -> 780/2000 episodes done
2024-05-05 06:38:30,269 Iter 133: steps this-iter 6000 total 798000 -> 780/2000 episodes done
2024-05-05 06:38:33,272 Iter 134: steps this-iter 6000 total 804000 -> 780/2000 episodes done
2024-05-05 06:38:36,365 Iter 135: steps this-iter 6000 total 810000 -> 810/2000 episodes done
2024-05-05 06:38:36,366 custom_metrics: {}
date: 2024-05-05_06-38-36
done: false
episode_len_mean: 1000.0
episode_reward_max: 15.63133044874178
episode_reward_mean: -20.260475650842526
episode_reward_min: -41.72320434162388
episodes_this_iter: 30
episodes_total: 810
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 990.176
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.27016979455947876
      entropy_coeff: 0.02500000037252903
      kl: 0.0015763974515721202
      model: {}
      policy_loss: -0.000451868399977684
      total_loss: 0.047702327370643616
      vf_explained_var: 0.3690105676651001
      vf_loss: 1.0981688499450684
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09690771996974945
      entropy_coeff: 1.969849944114685
      kl: 4.520607944868971e-06
      model: {}
      policy_loss: -1.5234574675559998e-05
      total_loss: -0.17616966366767883
      vf_explained_var: 0.07763995230197906
      vf_loss: 0.2947852611541748
  load_time_ms: 362.498
  num_steps_sampled: 810000
  num_steps_trained: 810000
  sample_time_ms: 1729.219
  update_time_ms: 6.504
iterations_since_restore: 130
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.675
  gpu_util_percent0: 0.0
  ram_util_percent: 29.900000000000002
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 16.648683483954034
  p: 15.035198166716501
policy_reward_mean:
  a: -5.924571710634561
  p: 3.4378111916957135
policy_reward_min:
  a: -14.022758922539023
  p: 0.33965466184057147
sampler_perf:
  mean_env_wait_ms: 3.032911301710627
  mean_inference_ms: 3.9403894541393574
  mean_processing_ms: 1.131930021824425
time_since_restore: 414.2623906135559
time_this_iter_s: 3.0898091793060303
time_total_s: 442.77145051956177
timestamp: 1714883916
timesteps_since_restore: 780000
timesteps_this_iter: 6000
timesteps_total: 810000
training_iteration: 135

2024-05-05 06:38:39,366 Iter 136: steps this-iter 6000 total 816000 -> 810/2000 episodes done
2024-05-05 06:38:42,420 Iter 137: steps this-iter 6000 total 822000 -> 810/2000 episodes done
2024-05-05 06:38:45,561 Iter 138: steps this-iter 6000 total 828000 -> 810/2000 episodes done
2024-05-05 06:38:48,565 Iter 139: steps this-iter 6000 total 834000 -> 810/2000 episodes done
2024-05-05 06:38:51,846 Iter 140: steps this-iter 6000 total 840000 -> 840/2000 episodes done
2024-05-05 06:38:51,848 custom_metrics: {}
date: 2024-05-05_06-38-51
done: false
episode_len_mean: 1000.0
episode_reward_max: 16.395166608636526
episode_reward_mean: -17.90336527024134
episode_reward_min: -40.30139248179721
episodes_this_iter: 30
episodes_total: 840
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 983.769
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3602215051651001
      entropy_coeff: 0.02500000037252903
      kl: 0.003065358381718397
      model: {}
      policy_loss: -0.0012584002688527107
      total_loss: 0.04391328990459442
      vf_explained_var: 0.44031164050102234
      vf_loss: 1.0835444927215576
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09691321849822998
      entropy_coeff: 1.9687249660491943
      kl: 3.5175019093003357e-06
      model: {}
      policy_loss: -8.965097367763519e-05
      total_loss: -0.14790844917297363
      vf_explained_var: 0.24301716685295105
      vf_loss: 0.8595340251922607
  load_time_ms: 359.589
  num_steps_sampled: 840000
  num_steps_trained: 840000
  sample_time_ms: 1733.841
  update_time_ms: 6.46
iterations_since_restore: 135
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.300000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 30.075
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.254971864922751
  p: 17.111807605695212
policy_reward_mean:
  a: -5.528506873266433
  p: 4.210662222824387
policy_reward_min:
  a: -13.38445178564099
  p: 0.3443093236781469
sampler_perf:
  mean_env_wait_ms: 3.0323031078057143
  mean_inference_ms: 3.9386013623392273
  mean_processing_ms: 1.1335176639997913
time_since_restore: 429.7330787181854
time_this_iter_s: 3.2792422771453857
time_total_s: 458.2421386241913
timestamp: 1714883931
timesteps_since_restore: 810000
timesteps_this_iter: 6000
timesteps_total: 840000
training_iteration: 140

2024-05-05 06:38:54,892 Iter 141: steps this-iter 6000 total 846000 -> 840/2000 episodes done
2024-05-05 06:38:58,205 Iter 142: steps this-iter 6000 total 852000 -> 840/2000 episodes done
2024-05-05 06:39:01,307 Iter 143: steps this-iter 6000 total 858000 -> 840/2000 episodes done
2024-05-05 06:39:04,363 Iter 144: steps this-iter 6000 total 864000 -> 840/2000 episodes done
2024-05-05 06:39:07,481 Iter 145: steps this-iter 6000 total 870000 -> 870/2000 episodes done
2024-05-05 06:39:07,483 custom_metrics: {}
date: 2024-05-05_06-39-07
done: false
episode_len_mean: 1000.0
episode_reward_max: 12.582732842012419
episode_reward_mean: -18.59852513196263
episode_reward_min: -39.84204827420993
episodes_this_iter: 30
episodes_total: 870
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 996.432
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.45182210206985474
      entropy_coeff: 0.02500000037252903
      kl: 0.003588832914829254
      model: {}
      policy_loss: -0.0006809383630752563
      total_loss: 0.04015194997191429
      vf_explained_var: 0.36134257912635803
      vf_loss: 1.0425688028335571
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09689465165138245
      entropy_coeff: 1.9675999879837036
      kl: 4.987707143300213e-06
      model: {}
      policy_loss: -2.466980367898941e-05
      total_loss: -0.1589306741952896
      vf_explained_var: 0.3473355770111084
      vf_loss: 0.6348779797554016
  load_time_ms: 360.868
  num_steps_sampled: 870000
  num_steps_trained: 870000
  sample_time_ms: 1736.15
  update_time_ms: 6.884
iterations_since_restore: 140
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 42.925000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 30.05
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 14.121292907617711
  p: 16.30528892397096
policy_reward_mean:
  a: -5.622152219191604
  p: 3.8900837448037797
policy_reward_min:
  a: -13.638267931542817
  p: 0.2832910155925644
sampler_perf:
  mean_env_wait_ms: 3.0334000343206595
  mean_inference_ms: 3.9386400876680354
  mean_processing_ms: 1.1340953678681351
time_since_restore: 445.3562397956848
time_this_iter_s: 3.115656614303589
time_total_s: 473.8652997016907
timestamp: 1714883947
timesteps_since_restore: 840000
timesteps_this_iter: 6000
timesteps_total: 870000
training_iteration: 145

2024-05-05 06:39:10,750 Iter 146: steps this-iter 6000 total 876000 -> 870/2000 episodes done
2024-05-05 06:39:13,847 Iter 147: steps this-iter 6000 total 882000 -> 870/2000 episodes done
2024-05-05 06:39:16,841 Iter 148: steps this-iter 6000 total 888000 -> 870/2000 episodes done
2024-05-05 06:39:20,128 Iter 149: steps this-iter 6000 total 894000 -> 870/2000 episodes done
2024-05-05 06:39:23,186 Iter 150: steps this-iter 6000 total 900000 -> 900/2000 episodes done
2024-05-05 06:39:23,188 custom_metrics: {}
date: 2024-05-05_06-39-23
done: false
episode_len_mean: 1000.0
episode_reward_max: -4.610457687641752
episode_reward_mean: -22.513996019159524
episode_reward_min: -38.45140164056835
episodes_this_iter: 30
episodes_total: 900
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1013.211
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.45434218645095825
      entropy_coeff: 0.02500000037252903
      kl: 0.0023434930481016636
      model: {}
      policy_loss: 0.00020791031420230865
      total_loss: 0.028478464111685753
      vf_explained_var: 0.3166413903236389
      vf_loss: 0.792582094669342
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09692222625017166
      entropy_coeff: 1.966475009918213
      kl: 3.1980039238987956e-06
      model: {}
      policy_loss: -5.476176738739014e-06
      total_loss: -0.1688491851091385
      vf_explained_var: 0.2857639193534851
      vf_loss: 0.43502873182296753
  load_time_ms: 354.724
  num_steps_sampled: 900000
  num_steps_trained: 900000
  sample_time_ms: 1745.661
  update_time_ms: 8.977
iterations_since_restore: 145
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 49.26666666666667
  gpu_util_percent0: 0.0
  ram_util_percent: 29.96666666666667
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 12.014755107325918
  p: 8.011882695379642
policy_reward_mean:
  a: -6.3947260252291445
  p: 3.0649080817570495
policy_reward_min:
  a: -13.382525603596704
  p: 0.3276426570114855
sampler_perf:
  mean_env_wait_ms: 3.0354971683059753
  mean_inference_ms: 3.9392404892898876
  mean_processing_ms: 1.1346717092517697
time_since_restore: 461.04979944229126
time_this_iter_s: 3.0556061267852783
time_total_s: 489.5588593482971
timestamp: 1714883963
timesteps_since_restore: 870000
timesteps_this_iter: 6000
timesteps_total: 900000
training_iteration: 150

2024-05-05 06:39:26,277 Iter 151: steps this-iter 6000 total 906000 -> 900/2000 episodes done
2024-05-05 06:39:29,392 Iter 152: steps this-iter 6000 total 912000 -> 900/2000 episodes done
2024-05-05 06:39:32,415 Iter 153: steps this-iter 6000 total 918000 -> 900/2000 episodes done
2024-05-05 06:39:35,551 Iter 154: steps this-iter 6000 total 924000 -> 900/2000 episodes done
2024-05-05 06:39:38,535 Iter 155: steps this-iter 6000 total 930000 -> 930/2000 episodes done
2024-05-05 06:39:38,537 custom_metrics: {}
date: 2024-05-05_06-39-38
done: false
episode_len_mean: 1000.0
episode_reward_max: -2.685556055071155
episode_reward_mean: -25.069173311901345
episode_reward_min: -46.607057817111304
episodes_this_iter: 30
episodes_total: 930
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 989.981
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.41583338379859924
      entropy_coeff: 0.02500000037252903
      kl: 0.001114209764637053
      model: {}
      policy_loss: 0.0003055054694414139
      total_loss: 0.046969570219516754
      vf_explained_var: 0.5028883218765259
      vf_loss: 1.141197919845581
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09692840278148651
      entropy_coeff: 1.9653500318527222
      kl: 1.840684035414597e-06
      model: {}
      policy_loss: 2.6283785700798035e-05
      total_loss: -0.180171400308609
      vf_explained_var: 0.29150739312171936
      vf_loss: 0.20601102709770203
  load_time_ms: 357.211
  num_steps_sampled: 930000
  num_steps_trained: 930000
  sample_time_ms: 1739.208
  update_time_ms: 8.427
iterations_since_restore: 150
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.325
  gpu_util_percent0: 0.0
  ram_util_percent: 30.224999999999998
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 18.75407431629114
  p: 6.319894974369561
policy_reward_mean:
  a: -6.815410242316914
  p: 2.1924676573663073
policy_reward_min:
  a: -18.411311277978825
  p: 0.16930932367821108
sampler_perf:
  mean_env_wait_ms: 3.035525652164472
  mean_inference_ms: 3.9378229986819595
  mean_processing_ms: 1.1353121213581412
time_since_restore: 476.3877217769623
time_this_iter_s: 2.9818100929260254
time_total_s: 504.89678168296814
timestamp: 1714883978
timesteps_since_restore: 900000
timesteps_this_iter: 6000
timesteps_total: 930000
training_iteration: 155

2024-05-05 06:39:41,730 Iter 156: steps this-iter 6000 total 936000 -> 930/2000 episodes done
2024-05-05 06:39:44,752 Iter 157: steps this-iter 6000 total 942000 -> 930/2000 episodes done
2024-05-05 06:39:48,014 Iter 158: steps this-iter 6000 total 948000 -> 930/2000 episodes done
2024-05-05 06:39:51,064 Iter 159: steps this-iter 6000 total 954000 -> 930/2000 episodes done
2024-05-05 06:39:54,088 Iter 160: steps this-iter 6000 total 960000 -> 960/2000 episodes done
2024-05-05 06:39:54,090 custom_metrics: {}
date: 2024-05-05_06-39-54
done: false
episode_len_mean: 1000.0
episode_reward_max: 9.444966830325509
episode_reward_mean: -21.996504760984482
episode_reward_min: -52.859235286422624
episodes_this_iter: 30
episodes_total: 960
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 968.148
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3850834369659424
      entropy_coeff: 0.02500000037252903
      kl: 0.0005193128017708659
      model: {}
      policy_loss: 0.00015605846419930458
      total_loss: 0.05036279559135437
      vf_explained_var: 0.10308574140071869
      vf_loss: 1.1966763734817505
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09695851802825928
      entropy_coeff: 1.9642250537872314
      kl: 2.3482496089854976e-06
      model: {}
      policy_loss: 4.8218294978141785e-05
      total_loss: -0.17522156238555908
      vf_explained_var: 0.17796100676059723
      vf_loss: 0.30357086658477783
  load_time_ms: 361.205
  num_steps_sampled: 960000
  num_steps_trained: 960000
  sample_time_ms: 1743.542
  update_time_ms: 6.801
iterations_since_restore: 155
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.25
  gpu_util_percent0: 0.0
  ram_util_percent: 30.025
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 17.555073307800065
  p: 9.194931108867054
policy_reward_mean:
  a: -6.403171770846484
  p: 3.616182322401453
policy_reward_min:
  a: -15.410955930856524
  p: 0.1693093236781499
sampler_perf:
  mean_env_wait_ms: 3.037320644782719
  mean_inference_ms: 3.9388292703779997
  mean_processing_ms: 1.1361608592102366
time_since_restore: 491.9300558567047
time_this_iter_s: 3.0222342014312744
time_total_s: 520.4391157627106
timestamp: 1714883994
timesteps_since_restore: 930000
timesteps_this_iter: 6000
timesteps_total: 960000
training_iteration: 160

2024-05-05 06:39:57,129 Iter 161: steps this-iter 6000 total 966000 -> 960/2000 episodes done
2024-05-05 06:40:00,189 Iter 162: steps this-iter 6000 total 972000 -> 960/2000 episodes done
2024-05-05 06:40:03,198 Iter 163: steps this-iter 6000 total 978000 -> 960/2000 episodes done
2024-05-05 06:40:06,433 Iter 164: steps this-iter 6000 total 984000 -> 960/2000 episodes done
2024-05-05 06:40:09,542 Iter 165: steps this-iter 6000 total 990000 -> 990/2000 episodes done
2024-05-05 06:40:09,545 custom_metrics: {}
date: 2024-05-05_06-40-09
done: false
episode_len_mean: 1000.0
episode_reward_max: 3.430124245010015
episode_reward_mean: -19.918692291956223
episode_reward_min: -44.00453815925035
episodes_this_iter: 30
episodes_total: 990
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 978.621
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.36587271094322205
      entropy_coeff: 0.02500000037252903
      kl: 0.0007585990242660046
      model: {}
      policy_loss: -0.00024116039276123047
      total_loss: 0.012211228720843792
      vf_explained_var: 0.40165382623672485
      vf_loss: 0.4319841265678406
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09688779711723328
      entropy_coeff: 1.9630999565124512
      kl: 2.20477522816509e-06
      model: {}
      policy_loss: 2.24430114030838e-05
      total_loss: -0.16558882594108582
      vf_explained_var: 0.30346807837486267
      vf_loss: 0.49178311228752136
  load_time_ms: 359.492
  num_steps_sampled: 990000
  num_steps_trained: 990000
  sample_time_ms: 1744.889
  update_time_ms: 6.956
iterations_since_restore: 160
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 39.85
  gpu_util_percent0: 0.0
  ram_util_percent: 30.075
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 12.770975502903163
  p: 11.661259087286592
policy_reward_mean:
  a: -6.161078163843468
  p: 4.725620363417645
policy_reward_min:
  a: -16.06063059367642
  p: 0.5461865233874529
sampler_perf:
  mean_env_wait_ms: 3.034285721306468
  mean_inference_ms: 3.9362421259575044
  mean_processing_ms: 1.1361869237321691
time_since_restore: 507.37245988845825
time_this_iter_s: 3.106985569000244
time_total_s: 535.8815197944641
timestamp: 1714884009
timesteps_since_restore: 960000
timesteps_this_iter: 6000
timesteps_total: 990000
training_iteration: 165

2024-05-05 06:40:12,870 Iter 166: steps this-iter 6000 total 996000 -> 990/2000 episodes done
2024-05-05 06:40:16,194 Iter 167: steps this-iter 6000 total 1002000 -> 990/2000 episodes done
2024-05-05 06:40:19,165 Iter 168: steps this-iter 6000 total 1008000 -> 990/2000 episodes done
2024-05-05 06:40:22,239 Iter 169: steps this-iter 6000 total 1014000 -> 990/2000 episodes done
2024-05-05 06:40:25,271 Iter 170: steps this-iter 6000 total 1020000 -> 1020/2000 episodes done
2024-05-05 06:40:25,273 custom_metrics: {}
date: 2024-05-05_06-40-25
done: false
episode_len_mean: 1000.0
episode_reward_max: 29.557769696203792
episode_reward_mean: -22.491276105322868
episode_reward_min: -49.717500000000015
episodes_this_iter: 30
episodes_total: 1020
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 980.11
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.372371107339859
      entropy_coeff: 0.02500000037252903
      kl: 0.0014293338172137737
      model: {}
      policy_loss: -0.0004550432786345482
      total_loss: 0.07504409551620483
      vf_explained_var: 0.18233315646648407
      vf_loss: 1.696168303489685
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0968952476978302
      entropy_coeff: 1.9619749784469604
      kl: 3.4755771594063845e-06
      model: {}
      policy_loss: -1.9213184714317322e-05
      total_loss: -0.1705789715051651
      vf_explained_var: 0.3757111430168152
      vf_loss: 0.39092594385147095
  load_time_ms: 355.759
  num_steps_sampled: 1020000
  num_steps_trained: 1020000
  sample_time_ms: 1757.744
  update_time_ms: 6.509
iterations_since_restore: 165
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.275
  gpu_util_percent0: 0.0
  ram_util_percent: 30.249999999999996
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 12.87920041247747
  p: 20.020691798400843
policy_reward_mean:
  a: -6.618393827860789
  p: 3.9822992061202798
policy_reward_min:
  a: -20.247412463627626
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.033246556497596
  mean_inference_ms: 3.9394329477124472
  mean_processing_ms: 1.1356396023596653
time_since_restore: 523.0339543819427
time_this_iter_s: 3.0310096740722656
time_total_s: 551.5430142879486
timestamp: 1714884025
timesteps_since_restore: 990000
timesteps_this_iter: 6000
timesteps_total: 1020000
training_iteration: 170

2024-05-05 06:40:28,320 Iter 171: steps this-iter 6000 total 1026000 -> 1020/2000 episodes done
2024-05-05 06:40:31,443 Iter 172: steps this-iter 6000 total 1032000 -> 1020/2000 episodes done
2024-05-05 06:40:34,468 Iter 173: steps this-iter 6000 total 1038000 -> 1020/2000 episodes done
2024-05-05 06:40:37,524 Iter 174: steps this-iter 6000 total 1044000 -> 1020/2000 episodes done
2024-05-05 06:40:40,679 Iter 175: steps this-iter 6000 total 1050000 -> 1050/2000 episodes done
2024-05-05 06:40:40,681 custom_metrics: {}
date: 2024-05-05_06-40-40
done: false
episode_len_mean: 1000.0
episode_reward_max: -8.55565609221781
episode_reward_mean: -28.92032728662422
episode_reward_min: -45.787537075702076
episodes_this_iter: 30
episodes_total: 1050
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 985.958
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.29787856340408325
      entropy_coeff: 0.02500000037252903
      kl: 0.0004519621143117547
      model: {}
      policy_loss: -0.0002953559160232544
      total_loss: 0.027968982234597206
      vf_explained_var: 0.22976800799369812
      vf_loss: 0.7142260074615479
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09694421291351318
      entropy_coeff: 1.9608500003814697
      kl: 1.9418819192651426e-06
      model: {}
      policy_loss: -2.2742897272109985e-06
      total_loss: -0.1783580780029297
      vf_explained_var: 0.1715724617242813
      vf_loss: 0.2347451001405716
  load_time_ms: 353.432
  num_steps_sampled: 1050000
  num_steps_trained: 1050000
  sample_time_ms: 1747.676
  update_time_ms: 6.822
iterations_since_restore: 170
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 42.875
  gpu_util_percent0: 0.0
  ram_util_percent: 30.05
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 7.804947282066038
  p: 9.250670988259564
policy_reward_mean:
  a: -7.988702469580824
  p: 3.0344825916990703
policy_reward_min:
  a: -14.723242559740811
  p: 0.2596546618405053
sampler_perf:
  mean_env_wait_ms: 3.031751476404636
  mean_inference_ms: 3.9373166834388442
  mean_processing_ms: 1.135883546056228
time_since_restore: 538.4141976833344
time_this_iter_s: 3.1517698764801025
time_total_s: 566.9232575893402
timestamp: 1714884040
timesteps_since_restore: 1020000
timesteps_this_iter: 6000
timesteps_total: 1050000
training_iteration: 175

2024-05-05 06:40:43,661 Iter 176: steps this-iter 6000 total 1056000 -> 1050/2000 episodes done
2024-05-05 06:40:46,674 Iter 177: steps this-iter 6000 total 1062000 -> 1050/2000 episodes done
2024-05-05 06:40:49,901 Iter 178: steps this-iter 6000 total 1068000 -> 1050/2000 episodes done
2024-05-05 06:40:52,944 Iter 179: steps this-iter 6000 total 1074000 -> 1050/2000 episodes done
2024-05-05 06:40:56,142 Iter 180: steps this-iter 6000 total 1080000 -> 1080/2000 episodes done
2024-05-05 06:40:56,144 custom_metrics: {}
date: 2024-05-05_06-40-56
done: false
episode_len_mean: 1000.0
episode_reward_max: 21.047069259759763
episode_reward_mean: -18.657499356272663
episode_reward_min: -36.72285328359979
episodes_this_iter: 30
episodes_total: 1080
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 987.634
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.35737302899360657
      entropy_coeff: 0.02500000037252903
      kl: 0.0001491847651777789
      model: {}
      policy_loss: -0.00013719871640205383
      total_loss: 0.038429755717515945
      vf_explained_var: 0.3558569848537445
      vf_loss: 0.9500255584716797
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0969754159450531
      entropy_coeff: 1.959725022315979
      kl: 1.7197960460180184e-06
      model: {}
      policy_loss: -3.350060433149338e-05
      total_loss: -0.14811226725578308
      vf_explained_var: 0.1996885985136032
      vf_loss: 0.8393270373344421
  load_time_ms: 360.192
  num_steps_sampled: 1080000
  num_steps_trained: 1080000
  sample_time_ms: 1720.014
  update_time_ms: 6.721
iterations_since_restore: 175
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.575
  gpu_util_percent0: 0.0
  ram_util_percent: 30.15
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 11.803616610312076
  p: 21.195011459870386
policy_reward_mean:
  a: -5.912715207657336
  p: 4.9933614743566785
policy_reward_min:
  a: -13.430472383806446
  p: 1.0593093236780098
sampler_perf:
  mean_env_wait_ms: 3.0311331093171097
  mean_inference_ms: 3.9356056132110018
  mean_processing_ms: 1.1366808056728048
time_since_restore: 553.8656098842621
time_this_iter_s: 3.196005344390869
time_total_s: 582.374669790268
timestamp: 1714884056
timesteps_since_restore: 1050000
timesteps_this_iter: 6000
timesteps_total: 1080000
training_iteration: 180

2024-05-05 06:40:59,208 Iter 181: steps this-iter 6000 total 1086000 -> 1080/2000 episodes done
2024-05-05 06:41:02,296 Iter 182: steps this-iter 6000 total 1092000 -> 1080/2000 episodes done
2024-05-05 06:41:05,343 Iter 183: steps this-iter 6000 total 1098000 -> 1080/2000 episodes done
2024-05-05 06:41:08,495 Iter 184: steps this-iter 6000 total 1104000 -> 1080/2000 episodes done
2024-05-05 06:41:11,519 Iter 185: steps this-iter 6000 total 1110000 -> 1110/2000 episodes done
2024-05-05 06:41:11,522 custom_metrics: {}
date: 2024-05-05_06-41-11
done: false
episode_len_mean: 1000.0
episode_reward_max: 14.419069500210576
episode_reward_mean: -20.217652877182143
episode_reward_min: -49.455
episodes_this_iter: 30
episodes_total: 1110
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 982.732
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3785495460033417
      entropy_coeff: 0.02500000037252903
      kl: 0.0005412059836089611
      model: {}
      policy_loss: -0.00029712822288274765
      total_loss: 0.02671981416642666
      vf_explained_var: 0.41928043961524963
      vf_loss: 0.7296136617660522
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09691015630960464
      entropy_coeff: 1.9586000442504883
      kl: 2.445363406877732e-06
      model: {}
      policy_loss: 3.478303551673889e-05
      total_loss: -0.17401057481765747
      vf_explained_var: 0.38256293535232544
      vf_loss: 0.31525781750679016
  load_time_ms: 365.896
  num_steps_sampled: 1110000
  num_steps_trained: 1110000
  sample_time_ms: 1717.268
  update_time_ms: 6.518
iterations_since_restore: 180
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.375
  gpu_util_percent0: 0.0
  ram_util_percent: 30.05
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 17.574839388456553
  p: 18.032766810953273
policy_reward_mean:
  a: -6.100719533600225
  p: 4.185225257218761
policy_reward_min:
  a: -15.550714246385184
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0289564430881977
  mean_inference_ms: 3.9331528943628036
  mean_processing_ms: 1.1368359430926145
time_since_restore: 569.2303607463837
time_this_iter_s: 3.0220203399658203
time_total_s: 597.7394206523895
timestamp: 1714884071
timesteps_since_restore: 1080000
timesteps_this_iter: 6000
timesteps_total: 1110000
training_iteration: 185

2024-05-05 06:41:14,817 Iter 186: steps this-iter 6000 total 1116000 -> 1110/2000 episodes done
2024-05-05 06:41:17,990 Iter 187: steps this-iter 6000 total 1122000 -> 1110/2000 episodes done
2024-05-05 06:41:20,997 Iter 188: steps this-iter 6000 total 1128000 -> 1110/2000 episodes done
2024-05-05 06:41:24,165 Iter 189: steps this-iter 6000 total 1134000 -> 1110/2000 episodes done
2024-05-05 06:41:27,162 Iter 190: steps this-iter 6000 total 1140000 -> 1140/2000 episodes done
2024-05-05 06:41:27,164 custom_metrics: {}
date: 2024-05-05_06-41-27
done: false
episode_len_mean: 1000.0
episode_reward_max: 22.096505989004836
episode_reward_mean: -13.867881718324451
episode_reward_min: -42.57491300577797
episodes_this_iter: 30
episodes_total: 1140
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1011.716
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.46199697256088257
      entropy_coeff: 0.02500000037252903
      kl: 0.0002759999770205468
      model: {}
      policy_loss: 9.494833648204803e-06
      total_loss: 0.058811526745557785
      vf_explained_var: 0.28894537687301636
      vf_loss: 1.4070391654968262
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09689932316541672
      entropy_coeff: 1.957474946975708
      kl: 1.8632902083481895e-06
      model: {}
      policy_loss: 1.9975006580352783e-05
      total_loss: -0.16413608193397522
      vf_explained_var: 0.283774733543396
      vf_loss: 0.5104387998580933
  load_time_ms: 356.992
  num_steps_sampled: 1140000
  num_steps_trained: 1140000
  sample_time_ms: 1710.843
  update_time_ms: 6.943
iterations_since_restore: 185
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 50.300000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 29.96666666666667
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 13.113915746219135
  p: 16.15034379947321
policy_reward_mean:
  a: -4.98263953548928
  p: 6.06267642363267
policy_reward_min:
  a: -17.232893574811897
  p: 0.2879166666695729
sampler_perf:
  mean_env_wait_ms: 3.0281601801153384
  mean_inference_ms: 3.9319467893797224
  mean_processing_ms: 1.137052061265597
time_since_restore: 584.8226053714752
time_this_iter_s: 2.9944679737091064
time_total_s: 613.3316652774811
timestamp: 1714884087
timesteps_since_restore: 1110000
timesteps_this_iter: 6000
timesteps_total: 1140000
training_iteration: 190

2024-05-05 06:41:30,261 Iter 191: steps this-iter 6000 total 1146000 -> 1140/2000 episodes done
2024-05-05 06:41:33,287 Iter 192: steps this-iter 6000 total 1152000 -> 1140/2000 episodes done
2024-05-05 06:41:36,353 Iter 193: steps this-iter 6000 total 1158000 -> 1140/2000 episodes done
2024-05-05 06:41:39,391 Iter 194: steps this-iter 6000 total 1164000 -> 1140/2000 episodes done
2024-05-05 06:41:42,635 Iter 195: steps this-iter 6000 total 1170000 -> 1170/2000 episodes done
2024-05-05 06:41:42,637 custom_metrics: {}
date: 2024-05-05_06-41-42
done: false
episode_len_mean: 1000.0
episode_reward_max: 15.017597862506832
episode_reward_mean: -16.45504847618768
episode_reward_min: -44.00823737237585
episodes_this_iter: 30
episodes_total: 1170
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1001.022
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4859636425971985
      entropy_coeff: 0.02500000037252903
      kl: 0.001339551294222474
      model: {}
      policy_loss: 0.0002482696436345577
      total_loss: 0.05004987120628357
      vf_explained_var: 0.42399609088897705
      vf_loss: 1.239013910293579
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09694556891918182
      entropy_coeff: 1.9563499689102173
      kl: 2.549585587985348e-06
      model: {}
      policy_loss: 6.042607128620148e-05
      total_loss: -0.16721972823143005
      vf_explained_var: 0.35095927119255066
      vf_loss: 0.4475862979888916
  load_time_ms: 359.235
  num_steps_sampled: 1170000
  num_steps_trained: 1170000
  sample_time_ms: 1729.786
  update_time_ms: 6.569
iterations_since_restore: 190
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 44.775
  gpu_util_percent0: 0.0
  ram_util_percent: 30.25
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.81697142152359
  p: 14.183106898942903
policy_reward_mean:
  a: -5.398669593631633
  p: 5.1396298983388435
policy_reward_min:
  a: -25.729601875131834
  p: 0.6663926570114457
sampler_perf:
  mean_env_wait_ms: 3.0288659657220958
  mean_inference_ms: 3.9321465859194356
  mean_processing_ms: 1.1375127139376373
time_since_restore: 600.2845485210419
time_this_iter_s: 3.2422730922698975
time_total_s: 628.7936084270477
timestamp: 1714884102
timesteps_since_restore: 1140000
timesteps_this_iter: 6000
timesteps_total: 1170000
training_iteration: 195

2024-05-05 06:41:45,698 Iter 196: steps this-iter 6000 total 1176000 -> 1170/2000 episodes done
2024-05-05 06:41:48,712 Iter 197: steps this-iter 6000 total 1182000 -> 1170/2000 episodes done
2024-05-05 06:41:51,936 Iter 198: steps this-iter 6000 total 1188000 -> 1170/2000 episodes done
2024-05-05 06:41:54,937 Iter 199: steps this-iter 6000 total 1194000 -> 1170/2000 episodes done
2024-05-05 06:41:58,090 Iter 200: steps this-iter 6000 total 1200000 -> 1200/2000 episodes done
2024-05-05 06:41:58,092 custom_metrics: {}
date: 2024-05-05_06-41-58
done: false
episode_len_mean: 1000.0
episode_reward_max: 15.105631805499597
episode_reward_mean: -26.17217574234705
episode_reward_min: -58.47924044695587
episodes_this_iter: 30
episodes_total: 1200
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 975.13
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.339868426322937
      entropy_coeff: 0.02500000037252903
      kl: 0.00112125463783741
      model: {}
      policy_loss: -0.0005397740751504898
      total_loss: 0.022730380296707153
      vf_explained_var: 0.47256770730018616
      vf_loss: 0.635337233543396
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09692361950874329
      entropy_coeff: 1.9552249908447266
      kl: 1.3036080872552702e-06
      model: {}
      policy_loss: 9.28528606891632e-06
      total_loss: -0.1841566562652588
      vf_explained_var: 0.40393537282943726
      vf_loss: 0.10683070123195648
  load_time_ms: 361.196
  num_steps_sampled: 1200000
  num_steps_trained: 1200000
  sample_time_ms: 1738.846
  update_time_ms: 6.376
iterations_since_restore: 195
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.599999999999994
  gpu_util_percent0: 0.0
  ram_util_percent: 29.925
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 18.761309555930588
  p: 12.27639364453345
policy_reward_mean:
  a: -7.580788585657975
  p: 4.150978600284847
policy_reward_min:
  a: -20.32935601097489
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0291204481939227
  mean_inference_ms: 3.9316601705674628
  mean_processing_ms: 1.138290540968847
time_since_restore: 615.7266745567322
time_this_iter_s: 3.150973081588745
time_total_s: 644.235734462738
timestamp: 1714884118
timesteps_since_restore: 1170000
timesteps_this_iter: 6000
timesteps_total: 1200000
training_iteration: 200

2024-05-05 06:41:58,177 >> Wrote dense logs to: ../../../runs/phase2/dense_logs/logs_0000000001200000
2024-05-05 06:42:01,259 Iter 201: steps this-iter 6000 total 1206000 -> 1200/2000 episodes done
2024-05-05 06:42:04,292 Iter 202: steps this-iter 6000 total 1212000 -> 1200/2000 episodes done
2024-05-05 06:42:07,374 Iter 203: steps this-iter 6000 total 1218000 -> 1200/2000 episodes done
2024-05-05 06:42:10,483 Iter 204: steps this-iter 6000 total 1224000 -> 1200/2000 episodes done
2024-05-05 06:42:13,729 Iter 205: steps this-iter 6000 total 1230000 -> 1230/2000 episodes done
2024-05-05 06:42:13,730 custom_metrics: {}
date: 2024-05-05_06-42-13
done: false
episode_len_mean: 1000.0
episode_reward_max: 6.518775792722666
episode_reward_mean: -26.28491505814894
episode_reward_min: -63.81750000000008
episodes_this_iter: 30
episodes_total: 1230
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 979.04
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.42093920707702637
      entropy_coeff: 0.02500000037252903
      kl: 0.0005592106026597321
      model: {}
      policy_loss: 0.00018076598644256592
      total_loss: 0.08589790016412735
      vf_explained_var: 0.31053873896598816
      vf_loss: 1.9248124361038208
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09690973907709122
      entropy_coeff: 1.9541000127792358
      kl: 2.9146933684387477e-06
      model: {}
      policy_loss: 8.909031748771667e-06
      total_loss: -0.18075180053710938
      vf_explained_var: 0.591900110244751
      vf_loss: 0.17221195995807648
  load_time_ms: 355.492
  num_steps_sampled: 1230000
  num_steps_trained: 1230000
  sample_time_ms: 1748.595
  update_time_ms: 6.744
iterations_since_restore: 200
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.15
  gpu_util_percent0: 0.0
  ram_util_percent: 30.15
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 14.666877095003912
  p: 16.25807217108643
policy_reward_mean:
  a: -7.355246482256455
  p: 3.1360708708768774
policy_reward_min:
  a: -21.807500000000005
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0337432461494274
  mean_inference_ms: 3.927818487099061
  mean_processing_ms: 1.137372185679031
time_since_restore: 631.2680261135101
time_this_iter_s: 3.243739604949951
time_total_s: 659.777086019516
timestamp: 1714884133
timesteps_since_restore: 1200000
timesteps_this_iter: 6000
timesteps_total: 1230000
training_iteration: 205

2024-05-05 06:42:17,080 Iter 206: steps this-iter 6000 total 1236000 -> 1230/2000 episodes done
2024-05-05 06:42:20,197 Iter 207: steps this-iter 6000 total 1242000 -> 1230/2000 episodes done
2024-05-05 06:42:23,327 Iter 208: steps this-iter 6000 total 1248000 -> 1230/2000 episodes done
2024-05-05 06:42:26,477 Iter 209: steps this-iter 6000 total 1254000 -> 1230/2000 episodes done
2024-05-05 06:42:29,628 Iter 210: steps this-iter 6000 total 1260000 -> 1260/2000 episodes done
2024-05-05 06:42:29,630 custom_metrics: {}
date: 2024-05-05_06-42-29
done: false
episode_len_mean: 1000.0
episode_reward_max: 6.2549483682335705
episode_reward_mean: -27.888211978759617
episode_reward_min: -67.59750000000011
episodes_this_iter: 30
episodes_total: 1260
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 974.906
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.38870564103126526
      entropy_coeff: 0.02500000037252903
      kl: 0.0004092371091246605
      model: {}
      policy_loss: -0.0002503730356693268
      total_loss: 0.0602218434214592
      vf_explained_var: 0.1355472058057785
      vf_loss: 1.4037971496582031
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0969247967004776
      entropy_coeff: 1.9529750347137451
      kl: 2.1152582121430896e-06
      model: {}
      policy_loss: 1.8656253814697266e-05
      total_loss: -0.17937853932380676
      vf_explained_var: 0.23546233773231506
      vf_loss: 0.19789007306098938
  load_time_ms: 353.103
  num_steps_sampled: 1260000
  num_steps_trained: 1260000
  sample_time_ms: 1799.946
  update_time_ms: 6.532
iterations_since_restore: 205
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 42.099999999999994
  gpu_util_percent0: 0.0
  ram_util_percent: 30.2
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 10.9945670702987
  p: 14.794446499682145
policy_reward_mean:
  a: -7.809361231648296
  p: 3.3492329478335616
policy_reward_min:
  a: -22.584999999999994
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0346368499530323
  mean_inference_ms: 3.9330096083660493
  mean_processing_ms: 1.1372324030665824
time_since_restore: 647.1556966304779
time_this_iter_s: 3.149567127227783
time_total_s: 675.6647565364838
timestamp: 1714884149
timesteps_since_restore: 1230000
timesteps_this_iter: 6000
timesteps_total: 1260000
training_iteration: 210

2024-05-05 06:42:32,695 Iter 211: steps this-iter 6000 total 1266000 -> 1260/2000 episodes done
2024-05-05 06:42:36,001 Iter 212: steps this-iter 6000 total 1272000 -> 1260/2000 episodes done
2024-05-05 06:42:39,134 Iter 213: steps this-iter 6000 total 1278000 -> 1260/2000 episodes done
2024-05-05 06:42:42,416 Iter 214: steps this-iter 6000 total 1284000 -> 1260/2000 episodes done
2024-05-05 06:42:45,459 Iter 215: steps this-iter 6000 total 1290000 -> 1290/2000 episodes done
2024-05-05 06:42:45,460 custom_metrics: {}
date: 2024-05-05_06-42-45
done: false
episode_len_mean: 1000.0
episode_reward_max: -3.237552609116216
episode_reward_mean: -30.791403381245075
episode_reward_min: -58.94923528642262
episodes_this_iter: 30
episodes_total: 1290
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 985.367
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3379696011543274
      entropy_coeff: 0.02500000037252903
      kl: 0.0003048861981369555
      model: {}
      policy_loss: 4.881387576460838e-05
      total_loss: 0.040245670825242996
      vf_explained_var: 0.3428153693675995
      vf_loss: 0.9729218482971191
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0969318374991417
      entropy_coeff: 1.9518500566482544
      kl: 2.4860244138835697e-06
      model: {}
      policy_loss: -7.718801498413086e-06
      total_loss: -0.18466795980930328
      vf_explained_var: 0.30553334951400757
      vf_loss: 0.09072332084178925
  load_time_ms: 364.975
  num_steps_sampled: 1290000
  num_steps_trained: 1290000
  sample_time_ms: 1805.209
  update_time_ms: 6.535
iterations_since_restore: 210
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.05
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 9.730997128349713
  p: 11.015306508084029
policy_reward_mean:
  a: -8.437979510347255
  p: 2.960514660143937
policy_reward_min:
  a: -21.545233226126726
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0365258630538365
  mean_inference_ms: 3.934397946495605
  mean_processing_ms: 1.1376859910853148
time_since_restore: 662.9750428199768
time_this_iter_s: 3.040905714035034
time_total_s: 691.4841027259827
timestamp: 1714884165
timesteps_since_restore: 1260000
timesteps_this_iter: 6000
timesteps_total: 1290000
training_iteration: 215

2024-05-05 06:42:48,547 Iter 216: steps this-iter 6000 total 1296000 -> 1290/2000 episodes done
2024-05-05 06:42:51,840 Iter 217: steps this-iter 6000 total 1302000 -> 1290/2000 episodes done
2024-05-05 06:42:54,950 Iter 218: steps this-iter 6000 total 1308000 -> 1290/2000 episodes done
2024-05-05 06:42:58,284 Iter 219: steps this-iter 6000 total 1314000 -> 1290/2000 episodes done
2024-05-05 06:43:01,404 Iter 220: steps this-iter 6000 total 1320000 -> 1320/2000 episodes done
2024-05-05 06:43:01,406 custom_metrics: {}
date: 2024-05-05_06-43-01
done: false
episode_len_mean: 1000.0
episode_reward_max: -0.9295643436405134
episode_reward_mean: -38.8811024928708
episode_reward_min: -65.8650000000001
episodes_this_iter: 30
episodes_total: 1320
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1001.177
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.36331504583358765
      entropy_coeff: 0.02500000037252903
      kl: 0.0004094558535143733
      model: {}
      policy_loss: -6.802938878536224e-05
      total_loss: 0.04500435292720795
      vf_explained_var: 0.3769815266132355
      vf_loss: 1.0831050872802734
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09693798422813416
      entropy_coeff: 1.9507249593734741
      kl: 2.757381025730865e-06
      model: {}
      policy_loss: -2.2023916244506836e-05
      total_loss: -0.1876671314239502
      vf_explained_var: 0.3412466049194336
      vf_loss: 0.029084548354148865
  load_time_ms: 386.345
  num_steps_sampled: 1320000
  num_steps_trained: 1320000
  sample_time_ms: 1772.223
  update_time_ms: 6.778
iterations_since_restore: 215
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.199999999999996
  gpu_util_percent0: 0.0
  ram_util_percent: 28.875000000000004
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.615429211587596
  p: 7.243149659753849
policy_reward_mean:
  a: -10.10041514694713
  p: 1.5205580949177189
policy_reward_min:
  a: -21.955
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.03871565957081
  mean_inference_ms: 3.935910883208055
  mean_processing_ms: 1.1379882461230737
time_since_restore: 678.9091191291809
time_this_iter_s: 3.118440866470337
time_total_s: 707.4181790351868
timestamp: 1714884181
timesteps_since_restore: 1290000
timesteps_this_iter: 6000
timesteps_total: 1320000
training_iteration: 220

2024-05-05 06:43:04,485 Iter 221: steps this-iter 6000 total 1326000 -> 1320/2000 episodes done
2024-05-05 06:43:07,655 Iter 222: steps this-iter 6000 total 1332000 -> 1320/2000 episodes done
2024-05-05 06:43:10,755 Iter 223: steps this-iter 6000 total 1338000 -> 1320/2000 episodes done
2024-05-05 06:43:14,027 Iter 224: steps this-iter 6000 total 1344000 -> 1320/2000 episodes done
2024-05-05 06:43:17,140 Iter 225: steps this-iter 6000 total 1350000 -> 1350/2000 episodes done
2024-05-05 06:43:17,142 custom_metrics: {}
date: 2024-05-05_06-43-17
done: false
episode_len_mean: 1000.0
episode_reward_max: -1.741901490877952
episode_reward_mean: -30.089940007218228
episode_reward_min: -65.43673528642263
episodes_this_iter: 30
episodes_total: 1350
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1002.459
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3992736339569092
      entropy_coeff: 0.02500000037252903
      kl: 0.0002696484443731606
      model: {}
      policy_loss: 0.00036871060729026794
      total_loss: 0.1149737536907196
      vf_explained_var: 0.29283440113067627
      vf_loss: 2.4917376041412354
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0969591736793518
      entropy_coeff: 1.9495999813079834
      kl: 3.424838041610201e-06
      model: {}
      policy_loss: 6.604194641113281e-05
      total_loss: -0.17875762283802032
      vf_explained_var: 0.4948630928993225
      vf_loss: 0.20415914058685303
  load_time_ms: 391.686
  num_steps_sampled: 1350000
  num_steps_trained: 1350000
  sample_time_ms: 1756.271
  update_time_ms: 6.461
iterations_since_restore: 220
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.425
  gpu_util_percent0: 0.0
  ram_util_percent: 29.025000000000002
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 15.878354055288042
  p: 9.122841667163856
policy_reward_mean:
  a: -8.139715784016223
  p: 2.4689231288466607
policy_reward_min:
  a: -28.705233226126722
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0390871478461103
  mean_inference_ms: 3.935466893453311
  mean_processing_ms: 1.1383520614309492
time_since_restore: 694.6333830356598
time_this_iter_s: 3.1117284297943115
time_total_s: 723.1424429416656
timestamp: 1714884197
timesteps_since_restore: 1320000
timesteps_this_iter: 6000
timesteps_total: 1350000
training_iteration: 225

2024-05-05 06:43:20,150 Iter 226: steps this-iter 6000 total 1356000 -> 1350/2000 episodes done
2024-05-05 06:43:23,487 Iter 227: steps this-iter 6000 total 1362000 -> 1350/2000 episodes done
2024-05-05 06:43:26,508 Iter 228: steps this-iter 6000 total 1368000 -> 1350/2000 episodes done
2024-05-05 06:43:29,564 Iter 229: steps this-iter 6000 total 1374000 -> 1350/2000 episodes done
2024-05-05 06:43:32,808 Iter 230: steps this-iter 6000 total 1380000 -> 1380/2000 episodes done
2024-05-05 06:43:32,810 custom_metrics: {}
date: 2024-05-05_06-43-32
done: false
episode_len_mean: 1000.0
episode_reward_max: 3.512076782256658
episode_reward_mean: -29.17140808360867
episode_reward_min: -52.39500000000009
episodes_this_iter: 30
episodes_total: 1380
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 996.575
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.40183037519454956
      entropy_coeff: 0.02500000037252903
      kl: 0.0008827376877889037
      model: {}
      policy_loss: 5.040410906076431e-05
      total_loss: 0.02393079735338688
      vf_explained_var: 0.28817594051361084
      vf_loss: 0.678523063659668
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09693348407745361
      entropy_coeff: 1.9484750032424927
      kl: 3.3830513075372437e-06
      model: {}
      policy_loss: -1.665949821472168e-05
      total_loss: -0.18439313769340515
      vf_explained_var: 0.3949960470199585
      vf_loss: 0.08991977572441101
  load_time_ms: 387.629
  num_steps_sampled: 1380000
  num_steps_trained: 1380000
  sample_time_ms: 1738.862
  update_time_ms: 5.975
iterations_since_restore: 225
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.375
  gpu_util_percent0: 0.0
  ram_util_percent: 29.025000000000002
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 18.994159118117743
  p: 12.61941861314186
policy_reward_mean:
  a: -7.884105791032905
  p: 2.3650150805229515
policy_reward_min:
  a: -27.491838709047926
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0394948473750416
  mean_inference_ms: 3.934970911180182
  mean_processing_ms: 1.1386005152418235
time_since_restore: 710.2896974086761
time_this_iter_s: 3.242002487182617
time_total_s: 738.798757314682
timestamp: 1714884212
timesteps_since_restore: 1350000
timesteps_this_iter: 6000
timesteps_total: 1380000
training_iteration: 230

2024-05-05 06:43:35,894 Iter 231: steps this-iter 6000 total 1386000 -> 1380/2000 episodes done
2024-05-05 06:43:39,156 Iter 232: steps this-iter 6000 total 1392000 -> 1380/2000 episodes done
2024-05-05 06:43:42,308 Iter 233: steps this-iter 6000 total 1398000 -> 1380/2000 episodes done
2024-05-05 06:43:45,366 Iter 234: steps this-iter 6000 total 1404000 -> 1380/2000 episodes done
2024-05-05 06:43:48,408 Iter 235: steps this-iter 6000 total 1410000 -> 1410/2000 episodes done
2024-05-05 06:43:48,410 custom_metrics: {}
date: 2024-05-05_06-43-48
done: false
episode_len_mean: 1000.0
episode_reward_max: -2.071677359779547
episode_reward_mean: -27.352572202362882
episode_reward_min: -64.34249999999997
episodes_this_iter: 30
episodes_total: 1410
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 998.403
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.444205641746521
      entropy_coeff: 0.02500000037252903
      kl: 0.0006930397357791662
      model: {}
      policy_loss: -0.0008542090654373169
      total_loss: 0.08637751638889313
      vf_explained_var: 0.20598645508289337
      vf_loss: 1.9667375087738037
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09694163501262665
      entropy_coeff: 1.947350025177002
      kl: 2.697333911783062e-06
      model: {}
      policy_loss: -3.0273571610450745e-05
      total_loss: -0.1816428303718567
      vf_explained_var: 0.4548947513103485
      vf_loss: 0.14333476126194
  load_time_ms: 375.984
  num_steps_sampled: 1410000
  num_steps_trained: 1410000
  sample_time_ms: 1733.954
  update_time_ms: 6.599
iterations_since_restore: 230
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 49.4
  gpu_util_percent0: 0.0
  ram_util_percent: 28.8
  vram_util_percent0: 0.007775606578733105
pid: 121304
policy_reward_max:
  a: 16.851735284693007
  p: 8.868604244385732
policy_reward_mean:
  a: -7.505867326214941
  p: 2.6708971024968684
policy_reward_min:
  a: -24.53032108003424
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0390113840711073
  mean_inference_ms: 3.9343048931684153
  mean_processing_ms: 1.1388302917042619
time_since_restore: 725.8769972324371
time_this_iter_s: 3.039644718170166
time_total_s: 754.386057138443
timestamp: 1714884228
timesteps_since_restore: 1380000
timesteps_this_iter: 6000
timesteps_total: 1410000
training_iteration: 235

2024-05-05 06:43:51,481 Iter 236: steps this-iter 6000 total 1416000 -> 1410/2000 episodes done
2024-05-05 06:43:54,519 Iter 237: steps this-iter 6000 total 1422000 -> 1410/2000 episodes done
2024-05-05 06:43:57,777 Iter 238: steps this-iter 6000 total 1428000 -> 1410/2000 episodes done
2024-05-05 06:44:00,853 Iter 239: steps this-iter 6000 total 1434000 -> 1410/2000 episodes done
2024-05-05 06:44:04,148 Iter 240: steps this-iter 6000 total 1440000 -> 1440/2000 episodes done
2024-05-05 06:44:04,150 custom_metrics: {}
date: 2024-05-05_06-44-04
done: false
episode_len_mean: 1000.0
episode_reward_max: 1.163298914384522
episode_reward_mean: -35.2767403995082
episode_reward_min: -63.45
episodes_this_iter: 30
episodes_total: 1440
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 988.526
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4303112328052521
      entropy_coeff: 0.02500000037252903
      kl: 0.0016702678985893726
      model: {}
      policy_loss: -0.0005484228022396564
      total_loss: 0.05002095177769661
      vf_explained_var: 0.22552594542503357
      vf_loss: 1.2265429496765137
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09686857461929321
      entropy_coeff: 1.9462250471115112
      kl: 3.055008164665196e-06
      model: {}
      policy_loss: -4.612840712070465e-05
      total_loss: -0.1832866072654724
      vf_explained_var: 0.40384402871131897
      vf_loss: 0.1057511642575264
  load_time_ms: 375.41
  num_steps_sampled: 1440000
  num_steps_trained: 1440000
  sample_time_ms: 1750.317
  update_time_ms: 7.495
iterations_since_restore: 235
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 43.75
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 18.93691011514082
  p: 14.542987354627062
policy_reward_mean:
  a: -9.367289773983021
  p: 2.1924186964238808
policy_reward_min:
  a: -20.495000000000005
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.04079724324659
  mean_inference_ms: 3.9354693874991837
  mean_processing_ms: 1.139264859427001
time_since_restore: 741.6043038368225
time_this_iter_s: 3.293051242828369
time_total_s: 770.1133637428284
timestamp: 1714884244
timesteps_since_restore: 1410000
timesteps_this_iter: 6000
timesteps_total: 1440000
training_iteration: 240

2024-05-05 06:44:07,249 Iter 241: steps this-iter 6000 total 1446000 -> 1440/2000 episodes done
2024-05-05 06:44:10,355 Iter 242: steps this-iter 6000 total 1452000 -> 1440/2000 episodes done
2024-05-05 06:44:13,400 Iter 243: steps this-iter 6000 total 1458000 -> 1440/2000 episodes done
2024-05-05 06:44:16,557 Iter 244: steps this-iter 6000 total 1464000 -> 1440/2000 episodes done
2024-05-05 06:44:19,651 Iter 245: steps this-iter 6000 total 1470000 -> 1470/2000 episodes done
2024-05-05 06:44:19,653 custom_metrics: {}
date: 2024-05-05_06-44-19
done: false
episode_len_mean: 1000.0
episode_reward_max: -12.552652639817158
episode_reward_mean: -33.833294396748634
episode_reward_min: -62.452500000000015
episodes_this_iter: 30
episodes_total: 1470
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 982.063
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.47629594802856445
      entropy_coeff: 0.02500000037252903
      kl: 0.00051278923638165
      model: {}
      policy_loss: 0.00043326523154973984
      total_loss: 0.059142619371414185
      vf_explained_var: 0.3075152635574341
      vf_loss: 1.4123350381851196
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.0969429686665535
      entropy_coeff: 1.945099949836731
      kl: 2.4047001261351397e-06
      model: {}
      policy_loss: -1.5087425708770752e-06
      total_loss: -0.18526607751846313
      vf_explained_var: 0.5675650835037231
      vf_loss: 0.06598402559757233
  load_time_ms: 386.267
  num_steps_sampled: 1470000
  num_steps_trained: 1470000
  sample_time_ms: 1737.21
  update_time_ms: 7.18
iterations_since_restore: 240
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.8
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 10.721344212482744
  p: 9.128435958649613
policy_reward_mean:
  a: -8.870886449809044
  p: 1.6502514024875485
policy_reward_min:
  a: -21.07249999999999
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.039061392608508
  mean_inference_ms: 3.933169876319775
  mean_processing_ms: 1.139645208161133
time_since_restore: 757.0974407196045
time_this_iter_s: 3.0924839973449707
time_total_s: 785.6065006256104
timestamp: 1714884259
timesteps_since_restore: 1440000
timesteps_this_iter: 6000
timesteps_total: 1470000
training_iteration: 245

2024-05-05 06:44:22,854 Iter 246: steps this-iter 6000 total 1476000 -> 1470/2000 episodes done
2024-05-05 06:44:25,944 Iter 247: steps this-iter 6000 total 1482000 -> 1470/2000 episodes done
2024-05-05 06:44:29,188 Iter 248: steps this-iter 6000 total 1488000 -> 1470/2000 episodes done
2024-05-05 06:44:32,396 Iter 249: steps this-iter 6000 total 1494000 -> 1470/2000 episodes done
2024-05-05 06:44:35,494 Iter 250: steps this-iter 6000 total 1500000 -> 1500/2000 episodes done
2024-05-05 06:44:35,495 custom_metrics: {}
date: 2024-05-05_06-44-35
done: false
episode_len_mean: 1000.0
episode_reward_max: -4.606783649484847
episode_reward_mean: -35.230188657678305
episode_reward_min: -70.72500000000002
episodes_this_iter: 30
episodes_total: 1500
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1011.237
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.546536922454834
      entropy_coeff: 0.02500000037252903
      kl: 0.00398217374458909
      model: {}
      policy_loss: -0.0007560127414762974
      total_loss: 0.06266273558139801
      vf_explained_var: 0.2928600609302521
      vf_loss: 1.5416433811187744
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09694768488407135
      entropy_coeff: 1.9439749717712402
      kl: 9.023233928928676e-07
      model: {}
      policy_loss: 4.824250936508179e-06
      total_loss: -0.18306219577789307
      vf_explained_var: 0.47693756222724915
      vf_loss: 0.10793689638376236
  load_time_ms: 382.804
  num_steps_sampled: 1500000
  num_steps_trained: 1500000
  sample_time_ms: 1721.875
  update_time_ms: 7.117
iterations_since_restore: 245
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.125
  gpu_util_percent0: 0.0
  ram_util_percent: 28.95
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 12.726889063624228
  p: 5.959191353512587
policy_reward_mean:
  a: -9.139649134681747
  p: 1.3284078810486875
policy_reward_min:
  a: -25.607499999999987
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.039525402029103
  mean_inference_ms: 3.93294308286668
  mean_processing_ms: 1.1399935342206082
time_since_restore: 772.9277291297913
time_this_iter_s: 3.096177577972412
time_total_s: 801.4367890357971
timestamp: 1714884275
timesteps_since_restore: 1470000
timesteps_this_iter: 6000
timesteps_total: 1500000
training_iteration: 250

2024-05-05 06:44:38,601 Iter 251: steps this-iter 6000 total 1506000 -> 1500/2000 episodes done
2024-05-05 06:44:41,756 Iter 252: steps this-iter 6000 total 1512000 -> 1500/2000 episodes done
2024-05-05 06:44:44,780 Iter 253: steps this-iter 6000 total 1518000 -> 1500/2000 episodes done
2024-05-05 06:44:47,980 Iter 254: steps this-iter 6000 total 1524000 -> 1500/2000 episodes done
2024-05-05 06:44:51,060 Iter 255: steps this-iter 6000 total 1530000 -> 1530/2000 episodes done
2024-05-05 06:44:51,062 custom_metrics: {}
date: 2024-05-05_06-44-51
done: false
episode_len_mean: 1000.0
episode_reward_max: -25.42904630854082
episode_reward_mean: -43.400633200293456
episode_reward_min: -68.50282145668012
episodes_this_iter: 30
episodes_total: 1530
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1015.723
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.32785436511039734
      entropy_coeff: 0.02500000037252903
      kl: 0.0008272379054687917
      model: {}
      policy_loss: -0.000373664777725935
      total_loss: 0.10036680102348328
      vf_explained_var: 0.19638362526893616
      vf_loss: 2.178736448287964
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.09697739779949188
      entropy_coeff: 1.9428499937057495
      kl: 3.5305235996929696e-06
      model: {}
      policy_loss: -2.9709190130233765e-06
      total_loss: -0.1848699301481247
      vf_explained_var: 0.4493240714073181
      vf_loss: 0.07091198861598969
  load_time_ms: 368.903
  num_steps_sampled: 1530000
  num_steps_trained: 1530000
  sample_time_ms: 1736.422
  update_time_ms: 6.968
iterations_since_restore: 250
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 39.6
  gpu_util_percent0: 0.0
  ram_util_percent: 28.799999999999997
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 7.2486559355772044
  p: 5.868685389831837
policy_reward_mean:
  a: -11.204581036133678
  p: 1.417690944241251
policy_reward_min:
  a: -29.642693524080897
  p: 0.12125000000303274
sampler_perf:
  mean_env_wait_ms: 3.0388019641594886
  mean_inference_ms: 3.9317947794155894
  mean_processing_ms: 1.1404856684513038
time_since_restore: 788.4811222553253
time_this_iter_s: 3.078019618988037
time_total_s: 816.9901821613312
timestamp: 1714884291
timesteps_since_restore: 1500000
timesteps_this_iter: 6000
timesteps_total: 1530000
training_iteration: 255

2024-05-05 06:44:54,270 Iter 256: steps this-iter 6000 total 1536000 -> 1530/2000 episodes done
2024-05-05 06:44:57,421 Iter 257: steps this-iter 6000 total 1542000 -> 1530/2000 episodes done
2024-05-05 06:45:00,486 Iter 258: steps this-iter 6000 total 1548000 -> 1530/2000 episodes done
2024-05-05 06:45:03,645 Iter 259: steps this-iter 6000 total 1554000 -> 1530/2000 episodes done
2024-05-05 06:45:06,699 Iter 260: steps this-iter 6000 total 1560000 -> 1560/2000 episodes done
2024-05-05 06:45:06,701 custom_metrics: {}
date: 2024-05-05_06-45-06
done: false
episode_len_mean: 1000.0
episode_reward_max: -4.9207545370981345
episode_reward_mean: -39.5866836466542
episode_reward_min: -62.662499999999994
episodes_this_iter: 30
episodes_total: 1560
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 993.895
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3526996970176697
      entropy_coeff: 0.02500000037252903
      kl: 0.0015915428521111608
      model: {}
      policy_loss: -0.0003326078876852989
      total_loss: 0.11241945624351501
      vf_explained_var: 0.3105795383453369
      vf_loss: 2.4313910007476807
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11258666217327118
      entropy_coeff: 1.9417250156402588
      kl: 3.007297891599592e-06
      model: {}
      policy_loss: -2.8835609555244446e-05
      total_loss: -0.2114386409521103
      vf_explained_var: 0.2783234119415283
      vf_loss: 0.1440507173538208
  load_time_ms: 368.935
  num_steps_sampled: 1560000
  num_steps_trained: 1560000
  sample_time_ms: 1738.029
  update_time_ms: 6.493
iterations_since_restore: 255
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.05
  gpu_util_percent0: 0.0
  ram_util_percent: 29.125
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 8.614193973581116
  p: 9.540046015351678
policy_reward_mean:
  a: -10.44701278434913
  p: 2.2013674907423124
policy_reward_min:
  a: -25.98162763277915
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0389749125656818
  mean_inference_ms: 3.931698112220771
  mean_processing_ms: 1.1409580227995282
time_since_restore: 804.1074600219727
time_this_iter_s: 3.052581548690796
time_total_s: 832.6165199279785
timestamp: 1714884306
timesteps_since_restore: 1530000
timesteps_this_iter: 6000
timesteps_total: 1560000
training_iteration: 260

2024-05-05 06:45:09,720 Iter 261: steps this-iter 6000 total 1566000 -> 1560/2000 episodes done
2024-05-05 06:45:12,883 Iter 262: steps this-iter 6000 total 1572000 -> 1560/2000 episodes done
2024-05-05 06:45:15,947 Iter 263: steps this-iter 6000 total 1578000 -> 1560/2000 episodes done
2024-05-05 06:45:19,265 Iter 264: steps this-iter 6000 total 1584000 -> 1560/2000 episodes done
2024-05-05 06:45:22,476 Iter 265: steps this-iter 6000 total 1590000 -> 1590/2000 episodes done
2024-05-05 06:45:22,478 custom_metrics: {}
date: 2024-05-05_06-45-22
done: false
episode_len_mean: 1000.0
episode_reward_max: -3.9331000199743045
episode_reward_mean: -34.68345719531239
episode_reward_min: -63.55499999999996
episodes_this_iter: 30
episodes_total: 1590
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 993.693
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4139068126678467
      entropy_coeff: 0.02500000037252903
      kl: 0.0006216031615622342
      model: {}
      policy_loss: -0.00010829558596014977
      total_loss: 0.09027992933988571
      vf_explained_var: 0.43249592185020447
      vf_loss: 2.0147178173065186
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11257709562778473
      entropy_coeff: 1.940600037574768
      kl: 3.0020642043382395e-06
      model: {}
      policy_loss: -8.014962077140808e-06
      total_loss: -0.21044129133224487
      vf_explained_var: 0.3100505471229553
      vf_loss: 0.1606770157814026
  load_time_ms: 374.101
  num_steps_sampled: 1590000
  num_steps_trained: 1590000
  sample_time_ms: 1755.535
  update_time_ms: 6.2
iterations_since_restore: 260
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.925
  gpu_util_percent0: 0.0
  ram_util_percent: 29.0
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 13.71086685055836
  p: 8.85950157792462
policy_reward_mean:
  a: -9.21003488830366
  p: 2.15668235790224
policy_reward_min:
  a: -27.253998807534174
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0389078655147435
  mean_inference_ms: 3.9321645472592754
  mean_processing_ms: 1.1408321970683757
time_since_restore: 819.8725743293762
time_this_iter_s: 3.209104537963867
time_total_s: 848.3816342353821
timestamp: 1714884322
timesteps_since_restore: 1560000
timesteps_this_iter: 6000
timesteps_total: 1590000
training_iteration: 265

2024-05-05 06:45:25,796 Iter 266: steps this-iter 6000 total 1596000 -> 1590/2000 episodes done
2024-05-05 06:45:29,218 Iter 267: steps this-iter 6000 total 1602000 -> 1590/2000 episodes done
2024-05-05 06:45:32,213 Iter 268: steps this-iter 6000 total 1608000 -> 1590/2000 episodes done
2024-05-05 06:45:35,505 Iter 269: steps this-iter 6000 total 1614000 -> 1590/2000 episodes done
2024-05-05 06:45:38,576 Iter 270: steps this-iter 6000 total 1620000 -> 1620/2000 episodes done
2024-05-05 06:45:38,578 custom_metrics: {}
date: 2024-05-05_06-45-38
done: false
episode_len_mean: 1000.0
episode_reward_max: -5.802905183576279
episode_reward_mean: -35.87479976305749
episode_reward_min: -53.41203885030065
episodes_this_iter: 30
episodes_total: 1620
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 982.411
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3094959259033203
      entropy_coeff: 0.02500000037252903
      kl: 0.0012482786551117897
      model: {}
      policy_loss: -0.0005668415687978268
      total_loss: 0.05482327193021774
      vf_explained_var: 0.16122576594352722
      vf_loss: 1.2625503540039062
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11260281503200531
      entropy_coeff: 1.9394750595092773
      kl: 1.7654194834904047e-06
      model: {}
      policy_loss: -1.601502299308777e-05
      total_loss: -0.21320079267024994
      vf_explained_var: 0.22772279381752014
      vf_loss: 0.10411178320646286
  load_time_ms: 377.419
  num_steps_sampled: 1620000
  num_steps_trained: 1620000
  sample_time_ms: 1810.262
  update_time_ms: 6.323
iterations_since_restore: 265
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.425000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 29.000000000000004
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 8.863759203049533
  p: 7.494479142203749
policy_reward_mean:
  a: -9.42051674449626
  p: 1.8072672149275415
policy_reward_min:
  a: -23.045204055107575
  p: 0.16417797102854112
sampler_perf:
  mean_env_wait_ms: 3.040621695015845
  mean_inference_ms: 3.9371746949089186
  mean_processing_ms: 1.1405223491171577
time_since_restore: 835.9614071846008
time_this_iter_s: 3.0689749717712402
time_total_s: 864.4704670906067
timestamp: 1714884338
timesteps_since_restore: 1590000
timesteps_this_iter: 6000
timesteps_total: 1620000
training_iteration: 270

2024-05-05 06:45:41,622 Iter 271: steps this-iter 6000 total 1626000 -> 1620/2000 episodes done
2024-05-05 06:45:44,897 Iter 272: steps this-iter 6000 total 1632000 -> 1620/2000 episodes done
2024-05-05 06:45:47,915 Iter 273: steps this-iter 6000 total 1638000 -> 1620/2000 episodes done
2024-05-05 06:45:51,156 Iter 274: steps this-iter 6000 total 1644000 -> 1620/2000 episodes done
2024-05-05 06:45:54,267 Iter 275: steps this-iter 6000 total 1650000 -> 1650/2000 episodes done
2024-05-05 06:45:54,268 custom_metrics: {}
date: 2024-05-05_06-45-54
done: false
episode_len_mean: 1000.0
episode_reward_max: -7.654429323421962
episode_reward_mean: -36.87420609422581
episode_reward_min: -56.73731902762359
episodes_this_iter: 30
episodes_total: 1650
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 983.861
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3875054121017456
      entropy_coeff: 0.02500000037252903
      kl: 0.0017502792179584503
      model: {}
      policy_loss: -0.0006667124107480049
      total_loss: 0.07874159514904022
      vf_explained_var: 0.34988442063331604
      vf_loss: 1.7819188833236694
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11257586628198624
      entropy_coeff: 1.938349962234497
      kl: 1.6481435523019172e-06
      model: {}
      policy_loss: -3.303308039903641e-05
      total_loss: -0.212356299161911
      vf_explained_var: 0.3641260266304016
      vf_loss: 0.11776350438594818
  load_time_ms: 379.062
  num_steps_sampled: 1650000
  num_steps_trained: 1650000
  sample_time_ms: 1798.022
  update_time_ms: 7.023
iterations_since_restore: 270
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.25
  gpu_util_percent0: 0.0
  ram_util_percent: 28.875
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 10.409086528713914
  p: 5.727108020179067
policy_reward_mean:
  a: -9.654068317789301
  p: 1.7420671769313796
policy_reward_min:
  a: -26.10158788418471
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0403705483379815
  mean_inference_ms: 3.936778269087057
  mean_processing_ms: 1.1413790884244175
time_since_restore: 851.6404247283936
time_this_iter_s: 3.108630418777466
time_total_s: 880.1494846343994
timestamp: 1714884354
timesteps_since_restore: 1620000
timesteps_this_iter: 6000
timesteps_total: 1650000
training_iteration: 275

2024-05-05 06:45:57,333 Iter 276: steps this-iter 6000 total 1656000 -> 1650/2000 episodes done
2024-05-05 06:46:00,620 Iter 277: steps this-iter 6000 total 1662000 -> 1650/2000 episodes done
2024-05-05 06:46:03,689 Iter 278: steps this-iter 6000 total 1668000 -> 1650/2000 episodes done
2024-05-05 06:46:06,966 Iter 279: steps this-iter 6000 total 1674000 -> 1650/2000 episodes done
2024-05-05 06:46:10,027 Iter 280: steps this-iter 6000 total 1680000 -> 1680/2000 episodes done
2024-05-05 06:46:10,029 custom_metrics: {}
date: 2024-05-05_06-46-10
done: false
episode_len_mean: 1000.0
episode_reward_max: -12.09178986995857
episode_reward_mean: -33.02276138183595
episode_reward_min: -50.09629793404709
episodes_this_iter: 30
episodes_total: 1680
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 998.243
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.42187437415122986
      entropy_coeff: 0.02500000037252903
      kl: 0.0006143482169136405
      model: {}
      policy_loss: 0.00010943971574306488
      total_loss: 0.030160782858729362
      vf_explained_var: 0.3662913143634796
      vf_loss: 0.8119639158248901
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11254879832267761
      entropy_coeff: 1.9372249841690063
      kl: 2.342652578590787e-06
      model: {}
      policy_loss: 2.9627233743667603e-05
      total_loss: -0.21398809552192688
      vf_explained_var: 0.5015240907669067
      vf_loss: 0.08029204607009888
  load_time_ms: 373.525
  num_steps_sampled: 1680000
  num_steps_trained: 1680000
  sample_time_ms: 1753.953
  update_time_ms: 7.182
iterations_since_restore: 275
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.9
  gpu_util_percent0: 0.0
  ram_util_percent: 28.875
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 6.226189673081052
  p: 12.754751716249723
policy_reward_mean:
  a: -8.785644983520893
  p: 2.119818552247623
policy_reward_min:
  a: -18.14315198182979
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.0404226075228586
  mean_inference_ms: 3.9362743675434597
  mean_processing_ms: 1.1415065405927318
time_since_restore: 867.387861251831
time_this_iter_s: 3.059328079223633
time_total_s: 895.8969211578369
timestamp: 1714884370
timesteps_since_restore: 1650000
timesteps_this_iter: 6000
timesteps_total: 1680000
training_iteration: 280

2024-05-05 06:46:13,370 Iter 281: steps this-iter 6000 total 1686000 -> 1680/2000 episodes done
2024-05-05 06:46:16,449 Iter 282: steps this-iter 6000 total 1692000 -> 1680/2000 episodes done
2024-05-05 06:46:19,626 Iter 283: steps this-iter 6000 total 1698000 -> 1680/2000 episodes done
2024-05-05 06:46:22,833 Iter 284: steps this-iter 6000 total 1704000 -> 1680/2000 episodes done
2024-05-05 06:46:25,895 Iter 285: steps this-iter 6000 total 1710000 -> 1710/2000 episodes done
2024-05-05 06:46:25,897 custom_metrics: {}
date: 2024-05-05_06-46-25
done: false
episode_len_mean: 1000.0
episode_reward_max: -1.6838361775013464
episode_reward_mean: -34.42045231893584
episode_reward_min: -50.09532709873055
episodes_this_iter: 30
episodes_total: 1710
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1005.776
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.322434663772583
      entropy_coeff: 0.02500000037252903
      kl: 0.0019609075970947742
      model: {}
      policy_loss: 0.0002402421087026596
      total_loss: 0.04000116139650345
      vf_explained_var: 0.2582068145275116
      vf_loss: 0.9564356803894043
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11256358027458191
      entropy_coeff: 1.9361000061035156
      kl: 4.411314421304269e-06
      model: {}
      policy_loss: -5.60469925403595e-06
      total_loss: -0.21476894617080688
      vf_explained_var: 0.4622262716293335
      vf_loss: 0.06341992318630219
  load_time_ms: 368.831
  num_steps_sampled: 1710000
  num_steps_trained: 1710000
  sample_time_ms: 1769.066
  update_time_ms: 6.671
iterations_since_restore: 280
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.225
  gpu_util_percent0: 0.0
  ram_util_percent: 28.95
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 10.440103105445658
  p: 10.01910354391555
policy_reward_mean:
  a: -8.957481075186575
  p: 1.409471981810452
policy_reward_min:
  a: -17.697238872778
  p: 3.1245061182749697e-12
sampler_perf:
  mean_env_wait_ms: 3.0420625876729845
  mean_inference_ms: 3.9373762382514186
  mean_processing_ms: 1.1419513167754134
time_since_restore: 883.2449808120728
time_this_iter_s: 3.0601868629455566
time_total_s: 911.7540407180786
timestamp: 1714884385
timesteps_since_restore: 1680000
timesteps_this_iter: 6000
timesteps_total: 1710000
training_iteration: 285

2024-05-05 06:46:29,208 Iter 286: steps this-iter 6000 total 1716000 -> 1710/2000 episodes done
2024-05-05 06:46:32,380 Iter 287: steps this-iter 6000 total 1722000 -> 1710/2000 episodes done
2024-05-05 06:46:35,481 Iter 288: steps this-iter 6000 total 1728000 -> 1710/2000 episodes done
2024-05-05 06:46:38,509 Iter 289: steps this-iter 6000 total 1734000 -> 1710/2000 episodes done
2024-05-05 06:46:41,598 Iter 290: steps this-iter 6000 total 1740000 -> 1740/2000 episodes done
2024-05-05 06:46:41,600 custom_metrics: {}
date: 2024-05-05_06-46-41
done: false
episode_len_mean: 1000.0
episode_reward_max: -15.593652679340037
episode_reward_mean: -34.738334236758156
episode_reward_min: -50.04505043683752
episodes_this_iter: 30
episodes_total: 1740
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1008.312
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3922176659107208
      entropy_coeff: 0.02500000037252903
      kl: 0.0022233091294765472
      model: {}
      policy_loss: 0.00032943394035100937
      total_loss: 0.025884762406349182
      vf_explained_var: 0.126944899559021
      vf_loss: 0.707215428352356
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11255952715873718
      entropy_coeff: 1.934975028038025
      kl: 1.8690335537030478e-06
      model: {}
      policy_loss: -1.944601535797119e-06
      total_loss: -0.21751061081886292
      vf_explained_var: 0.03296568989753723
      vf_loss: 0.005823992192745209
  load_time_ms: 366.736
  num_steps_sampled: 1740000
  num_steps_trained: 1740000
  sample_time_ms: 1763.218
  update_time_ms: 6.809
iterations_since_restore: 285
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.575
  gpu_util_percent0: 0.0
  ram_util_percent: 28.825
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 3.611422082780159
  p: 3.3671670052617215
policy_reward_mean:
  a: -8.871207820869358
  p: 0.7464970467192745
policy_reward_min:
  a: -19.315784649995834
  p: 3.1249633184678694e-12
sampler_perf:
  mean_env_wait_ms: 3.0427660720979666
  mean_inference_ms: 3.937507410900982
  mean_processing_ms: 1.142369094466902
time_since_restore: 898.9333591461182
time_this_iter_s: 3.0864450931549072
time_total_s: 927.442419052124
timestamp: 1714884401
timesteps_since_restore: 1710000
timesteps_this_iter: 6000
timesteps_total: 1740000
training_iteration: 290

2024-05-05 06:46:44,634 Iter 291: steps this-iter 6000 total 1746000 -> 1740/2000 episodes done
2024-05-05 06:46:47,902 Iter 292: steps this-iter 6000 total 1752000 -> 1740/2000 episodes done
2024-05-05 06:46:51,010 Iter 293: steps this-iter 6000 total 1758000 -> 1740/2000 episodes done
2024-05-05 06:46:54,258 Iter 294: steps this-iter 6000 total 1764000 -> 1740/2000 episodes done
2024-05-05 06:46:57,405 Iter 295: steps this-iter 6000 total 1770000 -> 1770/2000 episodes done
2024-05-05 06:46:57,407 custom_metrics: {}
date: 2024-05-05_06-46-57
done: false
episode_len_mean: 1000.0
episode_reward_max: -9.418325029460384
episode_reward_mean: -31.280321608028235
episode_reward_min: -36.16967699429748
episodes_this_iter: 30
episodes_total: 1770
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1002.123
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4525483250617981
      entropy_coeff: 0.02500000037252903
      kl: 0.00016905301890801638
      model: {}
      policy_loss: 6.402842700481415e-06
      total_loss: 0.02911369875073433
      vf_explained_var: 0.1918686330318451
      vf_loss: 0.8084200620651245
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.1125740110874176
      entropy_coeff: 1.9338500499725342
      kl: 2.460977384544094e-06
      model: {}
      policy_loss: -2.337619662284851e-05
      total_loss: -0.21520239114761353
      vf_explained_var: 0.5854107141494751
      vf_loss: 0.05044526606798172
  load_time_ms: 372.079
  num_steps_sampled: 1770000
  num_steps_trained: 1770000
  sample_time_ms: 1758.245
  update_time_ms: 6.795
iterations_since_restore: 290
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.15
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 3.776160215740269
  p: 7.176827469607102
policy_reward_mean:
  a: -8.10405949795292
  p: 1.1359163837834338
policy_reward_min:
  a: -16.655550832411535
  p: 3.1249633184678694e-12
sampler_perf:
  mean_env_wait_ms: 3.0436221643016874
  mean_inference_ms: 3.937825808421471
  mean_processing_ms: 1.1425382394103853
time_since_restore: 914.7287693023682
time_this_iter_s: 3.145358085632324
time_total_s: 943.237829208374
timestamp: 1714884417
timesteps_since_restore: 1740000
timesteps_this_iter: 6000
timesteps_total: 1770000
training_iteration: 295

2024-05-05 06:47:00,467 Iter 296: steps this-iter 6000 total 1776000 -> 1770/2000 episodes done
2024-05-05 06:47:03,621 Iter 297: steps this-iter 6000 total 1782000 -> 1770/2000 episodes done
2024-05-05 06:47:06,738 Iter 298: steps this-iter 6000 total 1788000 -> 1770/2000 episodes done
2024-05-05 06:47:09,928 Iter 299: steps this-iter 6000 total 1794000 -> 1770/2000 episodes done
2024-05-05 06:47:12,983 Iter 300: steps this-iter 6000 total 1800000 -> 1800/2000 episodes done
2024-05-05 06:47:12,985 custom_metrics: {}
date: 2024-05-05_06-47-12
done: false
episode_len_mean: 1000.0
episode_reward_max: -25.145498387831186
episode_reward_mean: -32.48847551196949
episode_reward_min: -46.63302783068479
episodes_this_iter: 30
episodes_total: 1800
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 997.622
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4091256260871887
      entropy_coeff: 0.02500000037252903
      kl: 0.0006623809458687901
      model: {}
      policy_loss: 0.00015784986317157745
      total_loss: 0.019232522696256638
      vf_explained_var: 0.22026827931404114
      vf_loss: 0.5860562324523926
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11255259066820145
      entropy_coeff: 1.932724952697754
      kl: 2.235183728771517e-06
      model: {}
      policy_loss: 6.0614198446273804e-05
      total_loss: -0.21721552312374115
      vf_explained_var: 0.4840852618217468
      vf_loss: 0.005141391884535551
  load_time_ms: 383.606
  num_steps_sampled: 1800000
  num_steps_trained: 1800000
  sample_time_ms: 1738.73
  update_time_ms: 7.037
iterations_since_restore: 295
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.075
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 1.909639402485439
  p: 2.288547318848318
policy_reward_mean:
  a: -8.303397597800727
  p: 0.7251148792334059
policy_reward_min:
  a: -19.836611467234775
  p: 0.12125000000303274
sampler_perf:
  mean_env_wait_ms: 3.0428277389697844
  mean_inference_ms: 3.9365700092504143
  mean_processing_ms: 1.1426889196572068
time_since_restore: 930.2946248054504
time_this_iter_s: 3.053283214569092
time_total_s: 958.8036847114563
timestamp: 1714884432
timesteps_since_restore: 1770000
timesteps_this_iter: 6000
timesteps_total: 1800000
training_iteration: 300

2024-05-05 06:47:13,070 >> Wrote dense logs to: ../../../runs/phase2/dense_logs/logs_0000000001800000
2024-05-05 06:47:16,356 Iter 301: steps this-iter 6000 total 1806000 -> 1800/2000 episodes done
2024-05-05 06:47:19,461 Iter 302: steps this-iter 6000 total 1812000 -> 1800/2000 episodes done
2024-05-05 06:47:22,785 Iter 303: steps this-iter 6000 total 1818000 -> 1800/2000 episodes done
2024-05-05 06:47:25,842 Iter 304: steps this-iter 6000 total 1824000 -> 1800/2000 episodes done
2024-05-05 06:47:29,124 Iter 305: steps this-iter 6000 total 1830000 -> 1830/2000 episodes done
2024-05-05 06:47:29,126 custom_metrics: {}
date: 2024-05-05_06-47-29
done: false
episode_len_mean: 1000.0
episode_reward_max: -26.050251644994873
episode_reward_mean: -34.61816991059414
episode_reward_min: -60.92999999999995
episodes_this_iter: 30
episodes_total: 1830
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 983.245
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.3893088698387146
      entropy_coeff: 0.02500000037252903
      kl: 0.0015371136832982302
      model: {}
      policy_loss: -0.0006941938772797585
      total_loss: 0.032280270010232925
      vf_explained_var: 0.2804044485092163
      vf_loss: 0.8541436791419983
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11250470578670502
      entropy_coeff: 1.9315999746322632
      kl: 3.063206122533302e-06
      model: {}
      policy_loss: -8.930638432502747e-05
      total_loss: -0.21713043749332428
      vf_explained_var: 0.38113832473754883
      vf_loss: 0.005459324456751347
  load_time_ms: 391.042
  num_steps_sampled: 1830000
  num_steps_trained: 1830000
  sample_time_ms: 1770.707
  update_time_ms: 6.939
iterations_since_restore: 300
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.175
  gpu_util_percent0: 0.0
  ram_util_percent: 28.924999999999997
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 7.319095385738407
  p: 4.449524348252703
policy_reward_mean:
  a: -8.822020717589014
  p: 0.6699129597618996
policy_reward_min:
  a: -19.176687857904597
  p: 0.0
sampler_perf:
  mean_env_wait_ms: 3.04949165544888
  mean_inference_ms: 3.93620872395835
  mean_processing_ms: 1.1421783978008326
time_since_restore: 946.3385078907013
time_this_iter_s: 3.28041934967041
time_total_s: 974.8475677967072
timestamp: 1714884449
timesteps_since_restore: 1800000
timesteps_this_iter: 6000
timesteps_total: 1830000
training_iteration: 305

2024-05-05 06:47:32,555 Iter 306: steps this-iter 6000 total 1836000 -> 1830/2000 episodes done
2024-05-05 06:47:35,655 Iter 307: steps this-iter 6000 total 1842000 -> 1830/2000 episodes done
2024-05-05 06:47:38,656 Iter 308: steps this-iter 6000 total 1848000 -> 1830/2000 episodes done
2024-05-05 06:47:41,906 Iter 309: steps this-iter 6000 total 1854000 -> 1830/2000 episodes done
2024-05-05 06:47:44,945 Iter 310: steps this-iter 6000 total 1860000 -> 1860/2000 episodes done
2024-05-05 06:47:44,947 custom_metrics: {}
date: 2024-05-05_06-47-44
done: false
episode_len_mean: 1000.0
episode_reward_max: -16.19134995850436
episode_reward_mean: -34.77894777770792
episode_reward_min: -50.08736814042974
episodes_this_iter: 30
episodes_total: 1860
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 972.157
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4046451449394226
      entropy_coeff: 0.02500000037252903
      kl: 0.0004830608959309757
      model: {}
      policy_loss: -0.00011459551751613617
      total_loss: 0.026614494621753693
      vf_explained_var: 0.20749053359031677
      vf_loss: 0.736904501914978
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11258843541145325
      entropy_coeff: 1.9304749965667725
      kl: 1.8942774886454572e-06
      model: {}
      policy_loss: -8.821487426757812e-06
      total_loss: -0.21709835529327393
      vf_explained_var: 0.5015661716461182
      vf_loss: 0.0051923273131251335
  load_time_ms: 383.466
  num_steps_sampled: 1860000
  num_steps_trained: 1860000
  sample_time_ms: 1814.765
  update_time_ms: 6.008
iterations_since_restore: 305
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 39.75
  gpu_util_percent0: 0.0
  ram_util_percent: 29.025
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 3.0671953664011085
  p: 4.486185717401397
policy_reward_mean:
  a: -8.843505089107994
  p: 0.5950725787240617
policy_reward_min:
  a: -17.88906538607668
  p: 3.1249633184678694e-12
sampler_perf:
  mean_env_wait_ms: 3.0499857888159436
  mean_inference_ms: 3.9387773812283893
  mean_processing_ms: 1.1416915479197804
time_since_restore: 962.1478474140167
time_this_iter_s: 3.0370006561279297
time_total_s: 990.6569073200226
timestamp: 1714884464
timesteps_since_restore: 1830000
timesteps_this_iter: 6000
timesteps_total: 1860000
training_iteration: 310

2024-05-05 06:47:48,004 Iter 311: steps this-iter 6000 total 1866000 -> 1860/2000 episodes done
2024-05-05 06:47:51,262 Iter 312: steps this-iter 6000 total 1872000 -> 1860/2000 episodes done
2024-05-05 06:47:54,361 Iter 313: steps this-iter 6000 total 1878000 -> 1860/2000 episodes done
2024-05-05 06:47:57,601 Iter 314: steps this-iter 6000 total 1884000 -> 1860/2000 episodes done
2024-05-05 06:48:00,691 Iter 315: steps this-iter 6000 total 1890000 -> 1890/2000 episodes done
2024-05-05 06:48:00,693 custom_metrics: {}
date: 2024-05-05_06-48-00
done: false
episode_len_mean: 1000.0
episode_reward_max: 1.0457719069195877
episode_reward_mean: -33.62040239640885
episode_reward_min: -45.559165748462625
episodes_this_iter: 30
episodes_total: 1890
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 982.54
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.36172574758529663
      entropy_coeff: 0.02500000037252903
      kl: 0.001095007755793631
      model: {}
      policy_loss: -3.377627581357956e-05
      total_loss: 0.04234743118286133
      vf_explained_var: 0.4235250949859619
      vf_loss: 1.028486967086792
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11257512867450714
      entropy_coeff: 1.9293500185012817
      kl: 1.6683397916494869e-06
      model: {}
      policy_loss: 2.7529895305633545e-06
      total_loss: -0.2062447965145111
      vf_explained_var: 0.42252078652381897
      vf_loss: 0.218985453248024
  load_time_ms: 377.594
  num_steps_sampled: 1890000
  num_steps_trained: 1890000
  sample_time_ms: 1779.046
  update_time_ms: 5.922
iterations_since_restore: 310
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.050000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 7.846025172777701
  p: 8.86278741140095
policy_reward_mean:
  a: -8.822279056312377
  p: 1.6687138288406516
policy_reward_min:
  a: -24.907014525592388
  p: 0.08465466184063536
sampler_perf:
  mean_env_wait_ms: 3.050386681162958
  mean_inference_ms: 3.9387637829036235
  mean_processing_ms: 1.1423144886023429
time_since_restore: 977.881157875061
time_this_iter_s: 3.0877814292907715
time_total_s: 1006.3902177810669
timestamp: 1714884480
timesteps_since_restore: 1860000
timesteps_this_iter: 6000
timesteps_total: 1890000
training_iteration: 315

2024-05-05 06:48:03,737 Iter 316: steps this-iter 6000 total 1896000 -> 1890/2000 episodes done
2024-05-05 06:48:06,953 Iter 317: steps this-iter 6000 total 1902000 -> 1890/2000 episodes done
2024-05-05 06:48:10,077 Iter 318: steps this-iter 6000 total 1908000 -> 1890/2000 episodes done
2024-05-05 06:48:13,315 Iter 319: steps this-iter 6000 total 1914000 -> 1890/2000 episodes done
2024-05-05 06:48:16,439 Iter 320: steps this-iter 6000 total 1920000 -> 1920/2000 episodes done
2024-05-05 06:48:16,441 custom_metrics: {}
date: 2024-05-05_06-48-16
done: false
episode_len_mean: 1000.0
episode_reward_max: -11.729097779462315
episode_reward_mean: -34.377096816646514
episode_reward_min: -50.05147609743441
episodes_this_iter: 30
episodes_total: 1920
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1012.047
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.46635693311691284
      entropy_coeff: 0.02500000037252903
      kl: 0.0009364348370581865
      model: {}
      policy_loss: -5.2438583225011826e-05
      total_loss: 0.027599357068538666
      vf_explained_var: 0.4418121576309204
      vf_loss: 0.7862144112586975
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11255761981010437
      entropy_coeff: 1.928225040435791
      kl: 8.265706128440797e-07
      model: {}
      policy_loss: -4.1816383600234985e-06
      total_loss: -0.2090320885181427
      vf_explained_var: 0.4883792996406555
      vf_loss: 0.16017034649848938
  load_time_ms: 369.852
  num_steps_sampled: 1920000
  num_steps_trained: 1920000
  sample_time_ms: 1742.029
  update_time_ms: 6.384
iterations_since_restore: 315
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.9
  gpu_util_percent0: 0.0
  ram_util_percent: 28.900000000000002
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 6.263211359419756
  p: 10.0798129420089
policy_reward_mean:
  a: -8.994191764741954
  p: 1.599670242321311
policy_reward_min:
  a: -20.840229890776612
  p: 0.08465466184063536
sampler_perf:
  mean_env_wait_ms: 3.050296252859555
  mean_inference_ms: 3.9381159169761855
  mean_processing_ms: 1.1425640419263765
time_since_restore: 993.5598568916321
time_this_iter_s: 3.1223151683807373
time_total_s: 1022.0689167976379
timestamp: 1714884496
timesteps_since_restore: 1890000
timesteps_this_iter: 6000
timesteps_total: 1920000
training_iteration: 320

2024-05-05 06:48:19,642 Iter 321: steps this-iter 6000 total 1926000 -> 1920/2000 episodes done
2024-05-05 06:48:22,848 Iter 322: steps this-iter 6000 total 1932000 -> 1920/2000 episodes done
2024-05-05 06:48:26,018 Iter 323: steps this-iter 6000 total 1938000 -> 1920/2000 episodes done
2024-05-05 06:48:29,010 Iter 324: steps this-iter 6000 total 1944000 -> 1920/2000 episodes done
2024-05-05 06:48:32,006 Iter 325: steps this-iter 6000 total 1950000 -> 1950/2000 episodes done
2024-05-05 06:48:32,008 custom_metrics: {}
date: 2024-05-05_06-48-32
done: false
episode_len_mean: 1000.0
episode_reward_max: -18.126938461451978
episode_reward_mean: -33.97887345711736
episode_reward_min: -52.82115979060542
episodes_this_iter: 30
episodes_total: 1950
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 1017.316
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.4748503267765045
      entropy_coeff: 0.02500000037252903
      kl: 0.001525186002254486
      model: {}
      policy_loss: 0.00030641909688711166
      total_loss: 0.029754236340522766
      vf_explained_var: 0.13442283868789673
      vf_loss: 0.826381504535675
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11258339881896973
      entropy_coeff: 1.9270999431610107
      kl: 2.5695585463836323e-06
      model: {}
      policy_loss: -1.6130506992340088e-06
      total_loss: -0.2163931131362915
      vf_explained_var: 0.5429474115371704
      vf_loss: 0.011359020136296749
  load_time_ms: 360.764
  num_steps_sampled: 1950000
  num_steps_trained: 1950000
  sample_time_ms: 1728.135
  update_time_ms: 6.371
iterations_since_restore: 320
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.050000000000004
  gpu_util_percent0: 0.0
  ram_util_percent: 28.85
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 1.0346051349805805
  p: 2.896809323677729
policy_reward_mean:
  a: -8.725996219017988
  p: 0.925111418954592
policy_reward_min:
  a: -26.312192762030463
  p: 0.12125000000303274
sampler_perf:
  mean_env_wait_ms: 3.0498751254003316
  mean_inference_ms: 3.937179558992487
  mean_processing_ms: 1.14284571418284
time_since_restore: 1009.1155068874359
time_this_iter_s: 2.9949119091033936
time_total_s: 1037.6245667934418
timestamp: 1714884512
timesteps_since_restore: 1920000
timesteps_this_iter: 6000
timesteps_total: 1950000
training_iteration: 325

2024-05-05 06:48:35,224 Iter 326: steps this-iter 6000 total 1956000 -> 1950/2000 episodes done
2024-05-05 06:48:38,256 Iter 327: steps this-iter 6000 total 1962000 -> 1950/2000 episodes done
2024-05-05 06:48:41,546 Iter 328: steps this-iter 6000 total 1968000 -> 1950/2000 episodes done
2024-05-05 06:48:44,678 Iter 329: steps this-iter 6000 total 1974000 -> 1950/2000 episodes done
2024-05-05 06:48:47,798 Iter 330: steps this-iter 6000 total 1980000 -> 1980/2000 episodes done
2024-05-05 06:48:47,800 custom_metrics: {}
date: 2024-05-05_06-48-47
done: false
episode_len_mean: 1000.0
episode_reward_max: -24.581923025902075
episode_reward_mean: -31.931158151633667
episode_reward_min: -39.96711526514873
episodes_this_iter: 30
episodes_total: 1980
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 993.956
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.493158221244812
      entropy_coeff: 0.02500000037252903
      kl: 0.0006535908905789256
      model: {}
      policy_loss: 0.0004521394148468971
      total_loss: 0.01954178884625435
      vf_explained_var: 0.19627773761749268
      vf_loss: 0.6283720135688782
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.11260588467121124
      entropy_coeff: 1.92597496509552
      kl: 2.777047484414652e-06
      model: {}
      policy_loss: -0.00013052672147750854
      total_loss: -0.21634319424629211
      vf_explained_var: 0.6427345871925354
      vf_loss: 0.013268493115901947
  load_time_ms: 371.806
  num_steps_sampled: 1980000
  num_steps_trained: 1980000
  sample_time_ms: 1752.972
  update_time_ms: 5.872
iterations_since_restore: 325
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 41.949999999999996
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 2.2300368536424884
  p: 3.075570628036489
policy_reward_mean:
  a: -8.16444602806349
  p: 0.7266259606202906
policy_reward_min:
  a: -16.817605247166085
  p: 0.1693093236781499
sampler_perf:
  mean_env_wait_ms: 3.050912218142827
  mean_inference_ms: 3.937934892497657
  mean_processing_ms: 1.1432457838778705
time_since_restore: 1024.8956916332245
time_this_iter_s: 3.1184580326080322
time_total_s: 1053.4047515392303
timestamp: 1714884527
timesteps_since_restore: 1950000
timesteps_this_iter: 6000
timesteps_total: 1980000
training_iteration: 330

2024-05-05 06:48:50,917 Iter 331: steps this-iter 6000 total 1986000 -> 1980/2000 episodes done
2024-05-05 06:48:53,946 Iter 332: steps this-iter 6000 total 1992000 -> 1980/2000 episodes done
2024-05-05 06:48:57,233 Iter 333: steps this-iter 6000 total 1998000 -> 1980/2000 episodes done
2024-05-05 06:49:00,432 Iter 334: steps this-iter 6000 total 2004000 -> 1980/2000 episodes done
2024-05-05 06:49:03,502 Iter 335: steps this-iter 6000 total 2010000 -> 2010/2000 episodes done
2024-05-05 06:49:03,504 custom_metrics: {}
date: 2024-05-05_06-49-03
done: false
episode_len_mean: 1000.0
episode_reward_max: -25.047426944337104
episode_reward_mean: -32.64514078028022
episode_reward_min: -45.5724946143201
episodes_this_iter: 30
episodes_total: 2010
experiment_id: 0e614256b1a446f58b94fea90e1202c2
hostname: wks
info:
  grad_time_ms: 995.862
  learner:
    a:
      cur_kl_coeff: 0.0
      cur_lr: 0.0003000000142492354
      entropy: 0.45780858397483826
      entropy_coeff: 0.02500000037252903
      kl: 0.00022429351520258933
      model: {}
      policy_loss: 6.109476089477539e-06
      total_loss: 0.024952739477157593
      vf_explained_var: 0.126609668135643
      vf_loss: 0.7278369665145874
    p:
      cur_kl_coeff: 0.0
      cur_lr: 9.999999747378752e-05
      entropy: 0.1124245822429657
      entropy_coeff: 1.9248499870300293
      kl: 4.104933395865373e-06
      model: {}
      policy_loss: -1.4496967196464539e-05
      total_loss: -0.21628496050834656
      vf_explained_var: 0.5745172500610352
      vf_loss: 0.002600358799099922
  load_time_ms: 378.867
  num_steps_sampled: 2010000
  num_steps_trained: 2010000
  sample_time_ms: 1757.734
  update_time_ms: 6.042
iterations_since_restore: 330
node_ip: 129.132.112.254
num_healthy_workers: 15
off_policy_estimator: {}
perf:
  cpu_util_percent: 40.775
  gpu_util_percent0: 0.0
  ram_util_percent: 28.975
  vram_util_percent0: 0.007775606578733106
pid: 121304
policy_reward_max:
  a: 2.7390804727463944
  p: 1.1456306521816517
policy_reward_mean:
  a: -8.258621940592752
  p: 0.38934698209078894
policy_reward_min:
  a: -17.71448080763696
  p: 3.1245061182749697e-12
sampler_perf:
  mean_env_wait_ms: 3.0509198173662857
  mean_inference_ms: 3.9379469842135753
  mean_processing_ms: 1.1432810501560478
time_since_restore: 1040.5883638858795
time_this_iter_s: 3.068399667739868
time_total_s: 1069.0974237918854
timestamp: 1714884543
timesteps_since_restore: 1980000
timesteps_this_iter: 6000
timesteps_total: 2010000
training_iteration: 335

2024-05-05 06:49:03,504 Completing! Saving final snapshot...


2024-05-05 06:49:03,647 Saved Trainer snapshot + Env object @ ../../../runs/phase2/ckpts/latest_checkpoint.pkl
2024-05-05 06:49:03,833 Saved TF weights @ ../../../runs/phase2/ckpts/agent.policy-model-weight-array.global-step-2010000
2024-05-05 06:49:04,002 Saved TF weights @ ../../../runs/phase2/ckpts/planner.policy-model-weight-array.global-step-2010000
2024-05-05 06:49:04,002 Final snapshot saved! All done.
